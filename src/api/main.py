import os
# è®¾ç½® HuggingFace é•œåƒç¯å¢ƒå˜é‡ï¼ˆè§£å†³ç½‘ç»œè¿æ¥é—®é¢˜ï¼‰
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import json
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_classic.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
import sys
from pathlib import Path
from typing import Optional, List, Iterator
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.CustomVLLM import CustomVLLM
from src.core.query_rewriter import QueryRewriter, create_query_rewriter
from src.core.reranker import Reranker, create_reranker
from src.api.monitoring import get_metrics_collector
import time

# é…ç½®
LAW_DB_DIR = str(project_root / "chroma_db")  # æ³•æ¡å‹çŸ¥è¯†åº“
CASE_DB_DIR = str(project_root / "chroma_db_case")  # æ¡ˆä¾‹å‹çŸ¥è¯†åº“
JUDGEMENT_DB_DIR = str(project_root / "chroma_db_judgement")  # åˆ¤å†³ä¹¦å‹çŸ¥è¯†åº“
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
# LLM æœåŠ¡çš„ç«¯å£æ˜¯ 8000ï¼ŒCustomVLLM é»˜è®¤æŒ‡å‘è¿™ä¸ªåœ°å€
VLLM_URL = os.getenv("VLLM_URL", "http://localhost:8000")

# åˆå§‹åŒ– LangChain ç»„ä»¶ (å…¨å±€åŠ è½½ä¸€æ¬¡)
app = FastAPI()
llm = CustomVLLM() # è¿æ¥åˆ°ä½ çš„ vLLM æœåŠ¡
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

# åˆå§‹åŒ–ç›‘æ§æŒ‡æ ‡æ”¶é›†å™¨
metrics_collector = get_metrics_collector(vllm_url=VLLM_URL)

# åˆå§‹åŒ– RAG ä¼˜åŒ–ç»„ä»¶
query_rewriter = None
reranker = None

# åˆå§‹åŒ– Query Rewriterï¼ˆæŸ¥è¯¢æ”¹å†™ï¼‰
try:
    query_rewriter = create_query_rewriter(llm=llm)
    print("âœ… Query Rewriter å·²åˆå§‹åŒ–")
except Exception as e:
    print(f"âš ï¸  Query Rewriter åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†è·³è¿‡æŸ¥è¯¢æ”¹å†™æ­¥éª¤")

# åˆå§‹åŒ– Rerankerï¼ˆé‡æ’åºï¼‰
try:
    reranker = create_reranker(model_name="BAAI/bge-reranker-base")
    print("âœ… Reranker å·²åˆå§‹åŒ–")
except Exception as e:
    print(f"âš ï¸  Reranker åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†è·³è¿‡é‡æ’åºæ­¥éª¤")

# åˆå§‹åŒ–å¤šä¸ªçŸ¥è¯†åº“ï¼ˆæ³•æ¡å‹ + æ¡ˆä¾‹å‹ + åˆ¤å†³ä¹¦å‹ï¼‰
law_vectordb: Optional[Chroma] = None
case_vectordb: Optional[Chroma] = None
judgement_vectordb: Optional[Chroma] = None
law_retriever = None
case_retriever = None
judgement_retriever = None

# åŠ è½½æ³•æ¡å‹çŸ¥è¯†åº“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
if Path(LAW_DB_DIR).exists() and any(Path(LAW_DB_DIR).iterdir()):
    try:
        law_vectordb = Chroma(persist_directory=LAW_DB_DIR, embedding_function=embeddings)
        law_retriever = law_vectordb.as_retriever(search_kwargs={"k": 2})
        print(f"âœ… æ³•æ¡å‹çŸ¥è¯†åº“å·²åŠ è½½: {LAW_DB_DIR}")
    except Exception as e:
        print(f"âš ï¸  æ³•æ¡å‹çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {e}")

# åŠ è½½æ¡ˆä¾‹å‹çŸ¥è¯†åº“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
if Path(CASE_DB_DIR).exists() and any(Path(CASE_DB_DIR).iterdir()):
    try:
        case_vectordb = Chroma(persist_directory=CASE_DB_DIR, embedding_function=embeddings)
        case_retriever = case_vectordb.as_retriever(search_kwargs={"k": 2})
        print(f"âœ… æ¡ˆä¾‹å‹çŸ¥è¯†åº“å·²åŠ è½½: {CASE_DB_DIR}")
    except Exception as e:
        print(f"âš ï¸  æ¡ˆä¾‹å‹çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {e}")

# åŠ è½½åˆ¤å†³ä¹¦å‹çŸ¥è¯†åº“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
if Path(JUDGEMENT_DB_DIR).exists() and any(Path(JUDGEMENT_DB_DIR).iterdir()):
    try:
        judgement_vectordb = Chroma(persist_directory=JUDGEMENT_DB_DIR, embedding_function=embeddings)
        judgement_retriever = judgement_vectordb.as_retriever(search_kwargs={"k": 1})
        print(f"âœ… åˆ¤å†³ä¹¦å‹çŸ¥è¯†åº“å·²åŠ è½½: {JUDGEMENT_DB_DIR}")
    except Exception as e:
        print(f"âš ï¸  åˆ¤å†³ä¹¦å‹çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {e}")

# é€‰æ‹©ä¸»è¦çš„çŸ¥è¯†åº“å’Œæ£€ç´¢å™¨
# ç»Ÿè®¡å¯ç”¨çš„çŸ¥è¯†åº“æ•°é‡
available_dbs = sum([
    law_vectordb is not None,
    case_vectordb is not None,
    judgement_vectordb is not None
])

if available_dbs >= 2:
    # å¤šä¸ªçŸ¥è¯†åº“ï¼Œä½¿ç”¨æ··åˆæ£€ç´¢
    vectordb = law_vectordb or case_vectordb or judgement_vectordb
    retriever = law_retriever or case_retriever or judgement_retriever
    db_names = []
    if law_vectordb:
        db_names.append("æ³•æ¡å‹")
    if case_vectordb:
        db_names.append("æ¡ˆä¾‹å‹")
    if judgement_vectordb:
        db_names.append("åˆ¤å†³ä¹¦å‹")
    print(f"ğŸ“š æ··åˆæ£€ç´¢æ¨¡å¼ï¼š{' + '.join(db_names)}")
elif judgement_vectordb:
    vectordb = judgement_vectordb
    retriever = judgement_retriever
    print("ğŸ“š ä½¿ç”¨åˆ¤å†³ä¹¦å‹çŸ¥è¯†åº“")
elif case_vectordb:
    vectordb = case_vectordb
    retriever = case_retriever
    print("ğŸ“š ä½¿ç”¨æ¡ˆä¾‹å‹çŸ¥è¯†åº“")
elif law_vectordb:
    vectordb = law_vectordb
    retriever = law_retriever
    print("ğŸ“š ä½¿ç”¨æ³•æ¡å‹çŸ¥è¯†åº“")
else:
    # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤è·¯å¾„
    try:
        vectordb = Chroma(persist_directory=LAW_DB_DIR, embedding_function=embeddings)
        retriever = vectordb.as_retriever(search_kwargs={"k": 3})
        print("ğŸ“š ä½¿ç”¨é»˜è®¤çŸ¥è¯†åº“è·¯å¾„")
    except Exception as e:
        print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½ä»»ä½•çŸ¥è¯†åº“: {e}")
        vectordb = None
        retriever = None

# å®šä¹‰ RAG æç¤ºè¯æ¨¡æ¿
# è¿™éƒ¨åˆ†å¾ˆé‡è¦ï¼Œå®ƒæŒ‡å¯¼ LLM å¦‚ä½•ä½¿ç”¨æ£€ç´¢åˆ°çš„çŸ¥è¯†
RAG_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ³•å¾‹åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ã€ä¸Šä¸‹æ–‡ã€‘æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
ä¸Šä¸‹æ–‡å¯èƒ½åŒ…å«æ³•å¾‹æ¡æ–‡æˆ–ç›¸å…³æ¡ˆä¾‹ã€‚è¯·ç»“åˆè¿™äº›ä¿¡æ¯ç»™å‡ºå‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ã€‚
å¦‚æœä½ æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´æ˜ä½ æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ ã€‚

ã€ä¸Šä¸‹æ–‡ã€‘ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åŸºäºä¸Šä¸‹æ–‡ä¸­çš„æ³•å¾‹æ¡æ–‡å’Œæ¡ˆä¾‹ï¼Œç»™å‡ºè¯¦ç»†ã€å‡†ç¡®çš„æ³•å¾‹å»ºè®®ã€‚
"""
RAG_PROMPT = PromptTemplate(
    template=RAG_PROMPT_TEMPLATE, input_variables=["context", "question"]
)

# å°è£… RAG é“¾ (Chain)
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff", # å°†æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£å—å¡«å……åˆ°ä¸Šä¸‹æ–‡
    retriever=retriever,
    chain_type_kwargs={"prompt": RAG_PROMPT}
)

# å®šä¹‰ API è¯·æ±‚ä½“
class ChatRequest(BaseModel):
    query: str
    temperature: float = 0.1
    max_tokens: int = 1024
    stream: bool = False  # æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º

# å®šä¹‰ API æ¥å£
@app.post("/api/rag/chat")
async def chat_endpoint(request: ChatRequest):
    """
    RAG èŠå¤©æ¥å£ï¼Œå®Œæ•´çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæµç¨‹ï¼š
    1. Query Rewrite: æ”¹å†™ç”¨æˆ·é—®é¢˜ä¸ºä¸“ä¸šæ£€ç´¢å…³é”®è¯
    2. Retrieve: å‘é‡æ£€ç´¢è·å– Top 50 æ–‡æ¡£
    3. Rerank: ä½¿ç”¨ Cross-Encoder é‡æ’åºåˆ° Top 5
    4. Generate: LLM ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    """
    start_time = time.time()
    print(f"ğŸ“¥ æ”¶åˆ°æŸ¥è¯¢: {request.query}")
    
    if not retriever:
        latency = time.time() - start_time
        metrics_collector.record_request(latency, success=False)
        return {"response": "âŒ é”™è¯¯: çŸ¥è¯†åº“æœªåŠ è½½ï¼Œè¯·å…ˆè¿è¡Œ ingest.py æ„å»ºçŸ¥è¯†åº“"}
    
    # === æ­¥éª¤ 1: Query Rewrite (æŸ¥è¯¢æ”¹å†™) ===
    search_query = request.query
    if query_rewriter:
        try:
            search_query = query_rewriter.rewrite(request.query)
            print(f"ğŸ“ æŸ¥è¯¢å·²æ”¹å†™: '{request.query}' -> '{search_query}'")
        except Exception as e:
            print(f"âš ï¸  æŸ¥è¯¢æ”¹å†™å¤±è´¥ï¼Œä½¿ç”¨åŸæŸ¥è¯¢: {e}")
            search_query = request.query
    else:
        search_query = request.query
    
    # === æ­¥éª¤ 2: Retrieve (å‘é‡æ£€ç´¢) ===
    # å¦‚æœå¤šä¸ªçŸ¥è¯†åº“éƒ½å­˜åœ¨ï¼Œä½¿ç”¨æ··åˆæ£€ç´¢
    available_retrievers = []
    if law_retriever:
        available_retrievers.append(("æ³•æ¡", law_retriever, 50))  # å…ˆå– Top 50
    if case_retriever:
        available_retrievers.append(("æ¡ˆä¾‹", case_retriever, 50))
    if judgement_retriever:
        available_retrievers.append(("åˆ¤å†³ä¹¦", judgement_retriever, 50))
    
    all_docs = []
    retrieval_info = []
    
    if len(available_retrievers) >= 2:
        # å¤šçŸ¥è¯†åº“æ··åˆæ£€ç´¢
        try:
            for name, ret, k in available_retrievers:
                docs = ret.get_relevant_documents(search_query)
                all_docs.extend(docs[:k])
                retrieval_info.append(f"{name}: {len(docs)}")
            print(f"ğŸ” å‘é‡æ£€ç´¢å®Œæˆï¼ˆ{', '.join(retrieval_info)}ï¼‰ï¼Œå…± {len(all_docs)} ä¸ªæ–‡æ¡£")
        except Exception as e:
            print(f"âŒ æ··åˆæ£€ç´¢å¤±è´¥: {e}")
            return {"response": f"âŒ æ£€ç´¢å¤±è´¥: {str(e)}"}
    else:
        # å•ä¸ªçŸ¥è¯†åº“æ£€ç´¢
        try:
            if available_retrievers:
                name, ret, k = available_retrievers[0]
                docs = ret.get_relevant_documents(search_query)
                all_docs = docs[:k]
                retrieval_info.append(f"{name}: {len(docs)}")
                print(f"ğŸ” å‘é‡æ£€ç´¢å®Œæˆï¼Œå…± {len(all_docs)} ä¸ªæ–‡æ¡£")
            else:
                # é™çº§åˆ°æ ‡å‡† RAG é“¾
                try:
                    result = rag_chain.invoke(request.query)
                    return {"response": result['result']}
                except Exception as e:
                    return {"response": f"âŒ æ£€ç´¢å¤±è´¥: {str(e)}"}
        except Exception as e:
            print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            return {"response": f"âŒ æ£€ç´¢å¤±è´¥: {str(e)}"}
    
    if not all_docs:
        return {"response": "âŒ æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè¯·å°è¯•å…¶ä»–é—®é¢˜"}
    
    # === æ­¥éª¤ 3: Rerank (é‡æ’åº) ===
    # å°†æ–‡æ¡£è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ç”¨äºé‡æ’åº
    doc_contents = [doc.page_content for doc in all_docs]
    doc_metadata = [{"page_content": doc.page_content, "metadata": doc.metadata} for doc in all_docs]
    
    if reranker and len(doc_contents) > 5:
        try:
            # ä½¿ç”¨é‡æ’åºå™¨å¯¹æ–‡æ¡£è¿›è¡Œç²¾ç»†æ’åº
            reranked_docs = reranker.rerank_with_metadata(
                query=request.query,  # ä½¿ç”¨åŸå§‹æŸ¥è¯¢è¿›è¡Œé‡æ’åº
                documents_with_metadata=doc_metadata,
                top_k=5
            )
            print(f"ğŸ¯ é‡æ’åºå®Œæˆï¼Œä» {len(doc_contents)} ä¸ªæ–‡æ¡£ä¸­é€‰å‡º Top 5")
            # æå–é‡æ’åºåçš„æ–‡æ¡£å†…å®¹
            final_docs = [doc['page_content'] for doc in reranked_docs]
        except Exception as e:
            print(f"âš ï¸  é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ: {e}")
            # é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ Top 5
            final_docs = doc_contents[:5]
    else:
        # å¦‚æœæ²¡æœ‰é‡æ’åºå™¨æˆ–æ–‡æ¡£æ•°é‡è¾ƒå°‘ï¼Œç›´æ¥å– Top 5
        final_docs = doc_contents[:5]
        if reranker:
            print(f"â„¹ï¸  æ–‡æ¡£æ•°é‡è¾ƒå°‘ï¼ˆ{len(doc_contents)}ï¼‰ï¼Œè·³è¿‡é‡æ’åº")
    
    # === æ­¥éª¤ 4: Generate (ç”Ÿæˆç­”æ¡ˆ) ===
    try:
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([f"[æ–‡æ¡£ {i+1}]\n{doc}" for i, doc in enumerate(final_docs)])
        
        # ä½¿ç”¨æç¤ºè¯æ¨¡æ¿ç”Ÿæˆå›ç­”
        prompt = RAG_PROMPT.format(context=context, question=request.query)
        
        # å¦‚æœå¯ç”¨æµå¼è¾“å‡º
        if request.stream:
            return StreamingResponse(
                _stream_response(
                    llm=llm,
                    prompt=prompt,
                    temperature=request.temperature,
                    max_tokens=request.max_tokens,
                    sources=final_docs,
                    start_time=start_time
                ),
                media_type="text/event-stream"
            )
        else:
            # éæµå¼è¾“å‡º
            response = llm.invoke(prompt)
            
            print(f"âœ… RAG æµç¨‹å®Œæˆ: æ”¹å†™ â†’ æ£€ç´¢({len(all_docs)}) â†’ é‡æ’åº({len(final_docs)}) â†’ ç”Ÿæˆ")
            return {
                "response": response,
                "sources": [
                    {"content": doc[:200] + "..." if len(doc) > 200 else doc, "index": i+1}
                    for i, doc in enumerate(final_docs)
                ]
            }
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        if request.stream:
            # æµå¼è¾“å‡ºé”™è¯¯
            def error_stream():
                yield f"data: {json.dumps({'error': str(e)})}\n\n"
            return StreamingResponse(error_stream(), media_type="text/event-stream")
        else:
            return {"response": f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}", "sources": []}


def _stream_response(
    llm: CustomVLLM,
    prompt: str,
    temperature: float = 0.1,
    max_tokens: int = 1024,
    sources: List[str] = None,
    start_time: float = None
) -> Iterator[str]:
    """
    æµå¼å“åº”ç”Ÿæˆå™¨
    
    Args:
        llm: CustomVLLM å®ä¾‹
        prompt: æç¤ºè¯
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§ token æ•°
        sources: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        start_time: è¯·æ±‚å¼€å§‹æ—¶é—´ï¼ˆç”¨äºå»¶è¿Ÿç»Ÿè®¡ï¼‰
        
    Yields:
        str: SSE æ ¼å¼çš„æ•°æ®æµ
    """
    # å‘é€å¼€å§‹ä¿¡å·
    yield f"data: {json.dumps({'type': 'start'})}\n\n"
    
    # æµå¼ç”Ÿæˆ
    full_response = ""
    success = True
    try:
        for chunk in llm.stream(prompt, temperature=temperature, max_tokens=max_tokens):
            full_response += chunk
            # å‘é€æ–‡æœ¬å—
            yield f"data: {json.dumps({'type': 'chunk', 'text': chunk})}\n\n"
        
        # å‘é€ç»“æŸä¿¡å·å’Œæ¥æºä¿¡æ¯
        yield f"data: {json.dumps({'type': 'done', 'sources': sources or []})}\n\n"
    except Exception as e:
        success = False
        yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"
    finally:
        # è®°å½•å»¶è¿ŸæŒ‡æ ‡ï¼ˆæµå¼è¾“å‡ºï¼‰
        if start_time is not None:
            latency = time.time() - start_time
            metrics_collector.record_request(latency, success=success)


# å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼ˆå¢å¼ºç‰ˆï¼‰
@app.get("/health")
async def health_check():
    """
    å¢å¼ºçš„å¥åº·æ£€æŸ¥ç«¯ç‚¹
    æ£€æŸ¥ï¼švLLM è¿æ¥ã€çŸ¥è¯†åº“çŠ¶æ€ã€æœåŠ¡å¯ç”¨æ€§
    """
    health_status = {
        "status": "healthy",
        "service": "LegalFlash-RAG API",
        "timestamp": datetime.now().isoformat(),
        "checks": {}
    }
    
    # æ£€æŸ¥ vLLM æœåŠ¡
    vllm_health = metrics_collector.check_vllm_health()
    health_status["checks"]["vllm"] = vllm_health
    
    # æ£€æŸ¥çŸ¥è¯†åº“çŠ¶æ€
    knowledge_bases = {
        "law": Path(LAW_DB_DIR).exists() and any(Path(LAW_DB_DIR).iterdir()),
        "case": Path(CASE_DB_DIR).exists() and any(Path(CASE_DB_DIR).iterdir()),
        "judgement": Path(JUDGEMENT_DB_DIR).exists() and any(Path(JUDGEMENT_DB_DIR).iterdir())
    }
    health_status["checks"]["knowledge_bases"] = knowledge_bases
    health_status["checks"]["available_retrievers"] = sum([
        law_retriever is not None,
        case_retriever is not None,
        judgement_retriever is not None
    ])
    
    # æ£€æŸ¥ RAG ç»„ä»¶
    health_status["checks"]["components"] = {
        "query_rewriter": query_rewriter is not None,
        "reranker": reranker is not None,
        "embeddings": embeddings is not None,
        "llm": llm is not None
    }
    
    # å¦‚æœ vLLM ä¸å¯ç”¨ï¼Œæ ‡è®°ä¸ºä¸å¥åº·
    if vllm_health["status"] != "healthy":
        health_status["status"] = "degraded"
    
    # å¦‚æœæ²¡æœ‰å¯ç”¨çš„çŸ¥è¯†åº“ï¼Œæ ‡è®°ä¸ºä¸å¥åº·
    if health_status["checks"]["available_retrievers"] == 0:
        health_status["status"] = "unhealthy"
    
    return health_status


# ç›‘æ§æŒ‡æ ‡ç«¯ç‚¹
@app.get("/metrics")
async def get_metrics():
    """
    è·å–ç³»ç»Ÿç›‘æ§æŒ‡æ ‡
    åŒ…æ‹¬ï¼šGPU ä½¿ç”¨ç‡ã€å»¶è¿Ÿç»Ÿè®¡ã€ååé‡ã€CPU/å†…å­˜ä½¿ç”¨æƒ…å†µ
    """
    return metrics_collector.get_all_metrics()


# ç›‘æ§æŒ‡æ ‡ç«¯ç‚¹ï¼ˆPrometheus æ ¼å¼ï¼Œå¯é€‰ï¼‰
@app.get("/metrics/prometheus")
async def get_prometheus_metrics():
    """
    è·å– Prometheus æ ¼å¼çš„ç›‘æ§æŒ‡æ ‡
    """
    metrics = metrics_collector.get_all_metrics()
    
    # è½¬æ¢ä¸º Prometheus æ ¼å¼
    prometheus_lines = []
    
    # è¯·æ±‚ç»Ÿè®¡
    prometheus_lines.append(f'legalflash_rag_requests_total {metrics["requests"]["total"]}')
    prometheus_lines.append(f'legalflash_rag_requests_errors_total {metrics["requests"]["errors"]}')
    prometheus_lines.append(f'legalflash_rag_requests_success_rate {metrics["requests"]["success_rate"]}')
    
    # å»¶è¿Ÿç»Ÿè®¡
    latency = metrics["latency"]
    prometheus_lines.append(f'legalflash_rag_latency_avg_seconds {latency["avg"]}')
    prometheus_lines.append(f'legalflash_rag_latency_p95_seconds {latency["p95"]}')
    prometheus_lines.append(f'legalflash_rag_latency_p99_seconds {latency["p99"]}')
    
    # ååé‡
    throughput = metrics["throughput"]
    prometheus_lines.append(f'legalflash_rag_throughput_rps_1min {throughput["requests_per_second_1min"]}')
    
    # GPU æŒ‡æ ‡
    for gpu in metrics["gpu"]:
        idx = gpu["index"]
        prometheus_lines.append(f'legalflash_rag_gpu_memory_used_gb{{gpu="{idx}"}} {gpu["memory"]["used_gb"]}')
        prometheus_lines.append(f'legalflash_rag_gpu_utilization_percent{{gpu="{idx}"}} {gpu["utilization_percent"]}')
    
    return "\n".join(prometheus_lines)
