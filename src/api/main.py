import os
# è®¾ç½® HuggingFace é•œåƒç¯å¢ƒå˜é‡ï¼ˆè§£å†³ç½‘ç»œè¿æ¥é—®é¢˜ï¼‰
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

from fastapi import FastAPI
from pydantic import BaseModel
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_classic.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
import sys
from pathlib import Path
from typing import Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.CustomVLLM import CustomVLLM

# é…ç½®
LAW_DB_DIR = str(project_root / "chroma_db")  # æ³•æ¡å‹çŸ¥è¯†åº“
CASE_DB_DIR = str(project_root / "chroma_db_case")  # æ¡ˆä¾‹å‹çŸ¥è¯†åº“
JUDGEMENT_DB_DIR = str(project_root / "chroma_db_judgement")  # åˆ¤å†³ä¹¦å‹çŸ¥è¯†åº“
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
# LLM æœåŠ¡çš„ç«¯å£æ˜¯ 8000ï¼ŒCustomVLLM é»˜è®¤æŒ‡å‘è¿™ä¸ªåœ°å€

# åˆå§‹åŒ– LangChain ç»„ä»¶ (å…¨å±€åŠ è½½ä¸€æ¬¡)
app = FastAPI()
llm = CustomVLLM() # è¿æ¥åˆ°ä½ çš„ vLLM æœåŠ¡
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

# åˆå§‹åŒ–å¤šä¸ªçŸ¥è¯†åº“ï¼ˆæ³•æ¡å‹ + æ¡ˆä¾‹å‹ + åˆ¤å†³ä¹¦å‹ï¼‰
law_vectordb: Optional[Chroma] = None
case_vectordb: Optional[Chroma] = None
judgement_vectordb: Optional[Chroma] = None
law_retriever = None
case_retriever = None
judgement_retriever = None

# åŠ è½½æ³•æ¡å‹çŸ¥è¯†åº“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
if Path(LAW_DB_DIR).exists() and any(Path(LAW_DB_DIR).iterdir()):
    try:
        law_vectordb = Chroma(persist_directory=LAW_DB_DIR, embedding_function=embeddings)
        law_retriever = law_vectordb.as_retriever(search_kwargs={"k": 2})
        print(f"âœ… æ³•æ¡å‹çŸ¥è¯†åº“å·²åŠ è½½: {LAW_DB_DIR}")
    except Exception as e:
        print(f"âš ï¸  æ³•æ¡å‹çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {e}")

# åŠ è½½æ¡ˆä¾‹å‹çŸ¥è¯†åº“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
if Path(CASE_DB_DIR).exists() and any(Path(CASE_DB_DIR).iterdir()):
    try:
        case_vectordb = Chroma(persist_directory=CASE_DB_DIR, embedding_function=embeddings)
        case_retriever = case_vectordb.as_retriever(search_kwargs={"k": 2})
        print(f"âœ… æ¡ˆä¾‹å‹çŸ¥è¯†åº“å·²åŠ è½½: {CASE_DB_DIR}")
    except Exception as e:
        print(f"âš ï¸  æ¡ˆä¾‹å‹çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {e}")

# åŠ è½½åˆ¤å†³ä¹¦å‹çŸ¥è¯†åº“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
if Path(JUDGEMENT_DB_DIR).exists() and any(Path(JUDGEMENT_DB_DIR).iterdir()):
    try:
        judgement_vectordb = Chroma(persist_directory=JUDGEMENT_DB_DIR, embedding_function=embeddings)
        judgement_retriever = judgement_vectordb.as_retriever(search_kwargs={"k": 1})
        print(f"âœ… åˆ¤å†³ä¹¦å‹çŸ¥è¯†åº“å·²åŠ è½½: {JUDGEMENT_DB_DIR}")
    except Exception as e:
        print(f"âš ï¸  åˆ¤å†³ä¹¦å‹çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {e}")

# é€‰æ‹©ä¸»è¦çš„çŸ¥è¯†åº“å’Œæ£€ç´¢å™¨
# ç»Ÿè®¡å¯ç”¨çš„çŸ¥è¯†åº“æ•°é‡
available_dbs = sum([
    law_vectordb is not None,
    case_vectordb is not None,
    judgement_vectordb is not None
])

if available_dbs >= 2:
    # å¤šä¸ªçŸ¥è¯†åº“ï¼Œä½¿ç”¨æ··åˆæ£€ç´¢
    vectordb = law_vectordb or case_vectordb or judgement_vectordb
    retriever = law_retriever or case_retriever or judgement_retriever
    db_names = []
    if law_vectordb:
        db_names.append("æ³•æ¡å‹")
    if case_vectordb:
        db_names.append("æ¡ˆä¾‹å‹")
    if judgement_vectordb:
        db_names.append("åˆ¤å†³ä¹¦å‹")
    print(f"ğŸ“š æ··åˆæ£€ç´¢æ¨¡å¼ï¼š{' + '.join(db_names)}")
elif judgement_vectordb:
    vectordb = judgement_vectordb
    retriever = judgement_retriever
    print("ğŸ“š ä½¿ç”¨åˆ¤å†³ä¹¦å‹çŸ¥è¯†åº“")
elif case_vectordb:
    vectordb = case_vectordb
    retriever = case_retriever
    print("ğŸ“š ä½¿ç”¨æ¡ˆä¾‹å‹çŸ¥è¯†åº“")
elif law_vectordb:
    vectordb = law_vectordb
    retriever = law_retriever
    print("ğŸ“š ä½¿ç”¨æ³•æ¡å‹çŸ¥è¯†åº“")
else:
    # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤è·¯å¾„
    try:
        vectordb = Chroma(persist_directory=LAW_DB_DIR, embedding_function=embeddings)
        retriever = vectordb.as_retriever(search_kwargs={"k": 3})
        print("ğŸ“š ä½¿ç”¨é»˜è®¤çŸ¥è¯†åº“è·¯å¾„")
    except Exception as e:
        print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½ä»»ä½•çŸ¥è¯†åº“: {e}")
        vectordb = None
        retriever = None

# å®šä¹‰ RAG æç¤ºè¯æ¨¡æ¿
# è¿™éƒ¨åˆ†å¾ˆé‡è¦ï¼Œå®ƒæŒ‡å¯¼ LLM å¦‚ä½•ä½¿ç”¨æ£€ç´¢åˆ°çš„çŸ¥è¯†
RAG_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ³•å¾‹åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ã€ä¸Šä¸‹æ–‡ã€‘æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
ä¸Šä¸‹æ–‡å¯èƒ½åŒ…å«æ³•å¾‹æ¡æ–‡æˆ–ç›¸å…³æ¡ˆä¾‹ã€‚è¯·ç»“åˆè¿™äº›ä¿¡æ¯ç»™å‡ºå‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ã€‚
å¦‚æœä½ æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´æ˜ä½ æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ ã€‚

ã€ä¸Šä¸‹æ–‡ã€‘ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åŸºäºä¸Šä¸‹æ–‡ä¸­çš„æ³•å¾‹æ¡æ–‡å’Œæ¡ˆä¾‹ï¼Œç»™å‡ºè¯¦ç»†ã€å‡†ç¡®çš„æ³•å¾‹å»ºè®®ã€‚
"""
RAG_PROMPT = PromptTemplate(
    template=RAG_PROMPT_TEMPLATE, input_variables=["context", "question"]
)

# å°è£… RAG é“¾ (Chain)
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff", # å°†æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£å—å¡«å……åˆ°ä¸Šä¸‹æ–‡
    retriever=retriever,
    chain_type_kwargs={"prompt": RAG_PROMPT}
)

# å®šä¹‰ API è¯·æ±‚ä½“
class ChatRequest(BaseModel):
    query: str

# å®šä¹‰ API æ¥å£
@app.post("/api/rag/chat")
async def chat_endpoint(request: ChatRequest):
    """RAG èŠå¤©æ¥å£ï¼ŒæŸ¥è¯¢æ³•å¾‹çŸ¥è¯†åº“å¹¶è¿”å›ç»“æœï¼ˆæ”¯æŒæ··åˆæ£€ç´¢ï¼‰"""
    print(f"ğŸ“¥ æ”¶åˆ°æŸ¥è¯¢: {request.query}")
    
    if not retriever:
        return {"response": "âŒ é”™è¯¯: çŸ¥è¯†åº“æœªåŠ è½½ï¼Œè¯·å…ˆè¿è¡Œ ingest.py æ„å»ºçŸ¥è¯†åº“"}
    
    # å¦‚æœå¤šä¸ªçŸ¥è¯†åº“éƒ½å­˜åœ¨ï¼Œä½¿ç”¨æ··åˆæ£€ç´¢
    available_retrievers = []
    if law_retriever:
        available_retrievers.append(("æ³•æ¡", law_retriever, 2))
    if case_retriever:
        available_retrievers.append(("æ¡ˆä¾‹", case_retriever, 1))
    if judgement_retriever:
        available_retrievers.append(("åˆ¤å†³ä¹¦", judgement_retriever, 1))
    
    if len(available_retrievers) >= 2:
        try:
            # ä»å¤šä¸ªçŸ¥è¯†åº“åˆ†åˆ«æ£€ç´¢
            all_docs = []
            retrieval_info = []
            
            for name, ret, k in available_retrievers:
                docs = ret.get_relevant_documents(request.query)
                all_docs.extend(docs[:k])
                retrieval_info.append(f"{name}: {len(docs)}")
            
            # æ‰‹åŠ¨æ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([doc.page_content for doc in all_docs])
            
            # ä½¿ç”¨æç¤ºè¯æ¨¡æ¿ç”Ÿæˆå›ç­”
            prompt = RAG_PROMPT.format(context=context, question=request.query)
            response = llm.invoke(prompt)
            
            print(f"âœ… æ··åˆæ£€ç´¢å®Œæˆï¼ˆ{', '.join(retrieval_info)}ï¼‰")
            return {"response": response}
        except Exception as e:
            print(f"âŒ æ··åˆæ£€ç´¢å¤±è´¥: {e}")
            # é™çº§åˆ°å•ä¸ªçŸ¥è¯†åº“æ£€ç´¢
            pass
    
    # å•ä¸ªçŸ¥è¯†åº“ï¼Œä½¿ç”¨æ ‡å‡† RAG é“¾
    try:
        result = rag_chain.invoke(request.query)
        return {"response": result['result']}
    except Exception as e:
        return {"response": f"âŒ æ£€ç´¢å¤±è´¥: {str(e)}"}
