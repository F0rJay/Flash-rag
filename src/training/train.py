import torch
import yaml
import os
import json
from datetime import datetime
from pathlib import Path
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from peft import LoraConfig
from trl import SFTTrainer

# GPU ç›‘æ§å›è°ƒ
try:
    from .gpu_monitor import GPUMonitorCallback
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent))
    from gpu_monitor import GPUMonitorCallback

# === 0. è¯»å–é…ç½®æ–‡ä»¶å‡½æ•° ===
from pathlib import Path

# è·å–é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent.parent

def load_config(config_path=None):
    if config_path is None:
        config_path = project_root / "config" / "train_config.yaml"
    else:
        config_path = project_root / config_path
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

# åŠ è½½é…ç½®
cfg = load_config()
print(f"Loading configuration from {project_root / 'config' / 'train_config.yaml'}...")

# === 1. é‡åŒ–é…ç½® (ä» Config è¯»å–) ===
bnb_config = None
if cfg['quantization']['load_in_4bit']:
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16 if not cfg['training']['bf16'] else torch.bfloat16,
        bnb_4bit_use_double_quant=False,
    )

# === 2. åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ ===
print(f"Loading model: {cfg['model']['name']}")
model = AutoModelForCausalLM.from_pretrained(
    cfg['model']['name'],
    quantization_config=bnb_config,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(cfg['model']['name'], trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# === 3. å‡†å¤‡ LoRA é…ç½® ===
peft_config = LoraConfig(
    r=cfg['lora']['r'],
    lora_alpha=cfg['lora']['lora_alpha'],
    lora_dropout=cfg['lora']['lora_dropout'],
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=cfg['lora']['target_modules']
)

# === 4. åŠ è½½æ•°æ®é›† ===
print(f"Loading training data from: {cfg['data']['train_path']}")
train_dataset = load_dataset("json", data_files=cfg['data']['train_path'], split="train")

# åŠ è½½éªŒè¯é›†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
eval_dataset = None
if cfg['data'].get('val_path') and Path(cfg['data']['val_path']).exists():
    print(f"Loading validation data from: {cfg['data']['val_path']}")
    eval_dataset = load_dataset("json", data_files=cfg['data']['val_path'], split="train")
else:
    print("âš ï¸  éªŒè¯é›†ä¸å­˜åœ¨ï¼Œå°†è·³è¿‡è¯„ä¼°")

# === 5. è®­ç»ƒå‚æ•°è®¾ç½® ===
# åˆ›å»ºæ—¥å¿—ç›®å½•
logging_dir = cfg['training'].get('logging_dir', os.path.join(cfg['training']['output_dir'], 'logs'))
os.makedirs(logging_dir, exist_ok=True)

# è·å–è¯„ä¼°å’Œä¿å­˜å‚æ•°
eval_steps = cfg['training'].get('eval_steps', 0)
save_steps = cfg['training']['save_steps']
load_best_model_at_end = cfg['training'].get('load_best_model_at_end', False)
do_eval = cfg['training'].get('do_eval', False) and eval_dataset is not None
eval_strategy = cfg['training'].get('eval_strategy', cfg['training'].get('evaluation_strategy', 'no'))

# å¦‚æœå¯ç”¨ load_best_model_at_end ä¸”ä½¿ç”¨ steps è¯„ä¼°ç­–ç•¥ï¼Œéœ€è¦ç¡®ä¿ save_steps æ˜¯ eval_steps çš„å€æ•°
if load_best_model_at_end and do_eval and eval_strategy == 'steps' and eval_steps > 0:
    if save_steps % eval_steps != 0:
        # è‡ªåŠ¨è°ƒæ•´ save_steps ä¸º eval_steps çš„å€æ•°
        if save_steps < eval_steps:
            # å¦‚æœ save_steps å°äº eval_stepsï¼Œè°ƒæ•´ä¸º eval_steps
            adjusted_save_steps = eval_steps
        else:
            # å¦‚æœ save_steps å¤§äº eval_stepsï¼Œè°ƒæ•´ä¸ºæœ€æ¥è¿‘çš„å€æ•°ï¼ˆå‘ä¸‹å–æ•´ï¼‰
            adjusted_save_steps = (save_steps // eval_steps) * eval_steps
            if adjusted_save_steps == 0:
                adjusted_save_steps = eval_steps
        
        print(f"âš ï¸  è‡ªåŠ¨è°ƒæ•´ save_steps: {save_steps} -> {adjusted_save_steps} (å¿…é¡»æ˜¯ eval_steps={eval_steps} çš„å€æ•°)")
        save_steps = adjusted_save_steps

training_args = TrainingArguments(
    output_dir=cfg['training']['output_dir'],
    num_train_epochs=cfg['training']['num_train_epochs'],
    per_device_train_batch_size=cfg['training']['per_device_train_batch_size'],
    per_device_eval_batch_size=cfg['training'].get('per_device_eval_batch_size', cfg['training']['per_device_train_batch_size']),
    gradient_accumulation_steps=cfg['training']['gradient_accumulation_steps'],
    optim=cfg['training']['optim'],
    save_steps=save_steps,
    logging_steps=cfg['training']['logging_steps'],
    eval_steps=eval_steps,
    do_eval=do_eval,
    eval_strategy=eval_strategy,
    load_best_model_at_end=load_best_model_at_end,
    metric_for_best_model=cfg['training'].get('metric_for_best_model', 'eval_loss'),
    greater_is_better=cfg['training'].get('metric_for_best_model', 'eval_loss') != 'eval_loss',
    learning_rate=float(cfg['training']['learning_rate']),
    weight_decay=0.001,
    fp16=cfg['training']['fp16'],
    bf16=cfg['training']['bf16'], # 5090 æ¨è True
    max_grad_norm=0.3,
    warmup_ratio=cfg['training']['warmup_ratio'],
    group_by_length=True,
    lr_scheduler_type="constant",
    # æ˜¾å­˜ä¼˜åŒ–ï¼šå¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆä»¥æ—¶é—´æ¢æ˜¾å­˜ï¼‰
    gradient_checkpointing=cfg['training'].get('gradient_checkpointing', True),
    # å¯è§†åŒ–è®¾ç½®
    report_to=cfg['training'].get('report_to', 'tensorboard'),
    logging_dir=logging_dir,
    # ä¿å­˜è®­ç»ƒå†å²
    save_total_limit=3,  # åªä¿ç•™æœ€è¿‘ 3 ä¸ªæ£€æŸ¥ç‚¹
    logging_first_step=True,
)

# === 6. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœé…ç½®äº†ï¼‰===
if cfg['training'].get('gradient_checkpointing', True):
    if hasattr(model, 'gradient_checkpointing_enable'):
        model.gradient_checkpointing_enable()
        print("âœ… å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼Œè®­ç»ƒé€Ÿåº¦ä¼šç¨æ…¢ï¼‰")
    else:
        print("âš ï¸  æ¨¡å‹ä¸æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹")

# === 7. åˆå§‹åŒ– SFTTrainer ä¸­çš„è‡ªå®šä¹‰æ•°æ®æ ¼å¼åŒ–å‡½æ•° ===
def formatting_prompts_func(example):
    # ç¡®ä¿ 'input' å­—æ®µå­˜åœ¨ï¼Œå³ä½¿æ˜¯ç©ºå­—ç¬¦ä¸²
    input_text = example.get('input', '')
    # Llama 3 æ ‡å‡†å¯¹è¯æ¨¡æ¿
    text = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{example['instruction']}\n{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n{example['output']}<|eot_id|>"
    return text

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=peft_config,
    args=training_args,
    formatting_func=formatting_prompts_func,
)

# === æ·»åŠ  GPU ç›‘æ§å›è°ƒ ===
if cfg['training'].get('gpu_monitor', {}).get('enabled', True):
    gpu_monitor = GPUMonitorCallback(
        log_interval=cfg['training'].get('gpu_monitor', {}).get('log_interval', 10),
        enable_tensorboard=cfg['training'].get('gpu_monitor', {}).get('enable_tensorboard', True)
    )
    trainer.add_callback(gpu_monitor)
    print("âœ… GPU ç›‘æ§å·²å¯ç”¨")

# === 7. å¼€å§‹è®­ç»ƒ ===
print("="*60)
print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
print(f"ğŸ“Š TensorBoard æ—¥å¿—ç›®å½•: {logging_dir}")
print(f"   å¯åŠ¨ TensorBoard: tensorboard --logdir {logging_dir}")
print("="*60)

# è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
train_start_time = datetime.now()

try:
    trainer.train()
    train_end_time = datetime.now()
    training_duration = (train_end_time - train_start_time).total_seconds() / 3600  # è½¬æ¢ä¸ºå°æ—¶
    
    print("\n" + "="*60)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"â±ï¸  è®­ç»ƒæ—¶é•¿: {training_duration:.2f} å°æ—¶")
    print("="*60)
    
    # ä¿å­˜è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
    training_stats = {
        "training_start": train_start_time.isoformat(),
        "training_end": train_end_time.isoformat(),
        "training_duration_hours": round(training_duration, 2),
        "total_steps": trainer.state.global_step,
        "total_epochs": trainer.state.epoch,
        "best_metric": trainer.state.best_metric if hasattr(trainer.state, 'best_metric') else None,
        "best_model_checkpoint": trainer.state.best_model_checkpoint if hasattr(trainer.state, 'best_model_checkpoint') else None,
    }
    
    stats_file = os.path.join(cfg['training']['output_dir'], 'training_stats.json')
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(training_stats, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ è®­ç»ƒç»Ÿè®¡å·²ä¿å­˜åˆ°: {stats_file}")
    
except Exception as e:
    print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    raise

# === 8. ä¿å­˜æ¨¡å‹ ===
# ä¿å­˜ LoRA é€‚é…å™¨
final_save_path = os.path.join(cfg['training']['output_dir'], cfg['model']['new_name'])
print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {final_save_path}...")
trainer.model.save_pretrained(final_save_path)
tokenizer.save_pretrained(final_save_path)
print("âœ… æ¨¡å‹ä¿å­˜å®Œæˆï¼")

# === 9. æç¤ºè¯„ä¼° ===
print("\n" + "="*60)
print("ğŸ“Š è®­ç»ƒå®Œæˆï¼Œå»ºè®®è¿›è¡Œæ¨¡å‹è¯„ä¼°ï¼š")
print(f"   python src/training/evaluate.py --model_path {final_save_path}")
print("="*60)