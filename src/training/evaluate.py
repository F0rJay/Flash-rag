#!/usr/bin/env python3
"""
æ¨¡å‹è¯„ä¼°è„šæœ¬
åŠŸèƒ½ï¼š
1. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½
2. è®¡ç®—å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼ˆBLEUã€ROUGEã€å›°æƒ‘åº¦ç­‰ï¼‰
3. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
4. ä¿å­˜è¯„ä¼°ç»“æœ

ä½¿ç”¨æ–¹æ³•ï¼š
    python src/training/evaluate.py --model_path output/llama3-law-assistant-lora [--test_path data/datasets/test.jsonl]
"""

import torch
import yaml
import json
import argparse
import numpy as np
from pathlib import Path
from datetime import datetime
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from peft import PeftModel
import os

# è¯„ä¼°æŒ‡æ ‡
try:
    from rouge_score import rouge_scorer
    ROUGE_AVAILABLE = True
except ImportError:
    ROUGE_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: rouge_score æœªå®‰è£…ï¼Œå°†è·³è¿‡ ROUGE æŒ‡æ ‡è®¡ç®—")
    print("   å®‰è£…: pip install rouge-score")

try:
    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
    from nltk.tokenize import word_tokenize
    import nltk
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt', quiet=True)
    BLEU_AVAILABLE = True
except ImportError:
    BLEU_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: nltk æœªå®‰è£…ï¼Œå°†è·³è¿‡ BLEU æŒ‡æ ‡è®¡ç®—")
    print("   å®‰è£…: pip install nltk")

# è·å–é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent.parent

def load_config(config_path=None):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if config_path is None:
        config_path = project_root / "config" / "train_config.yaml"
    else:
        config_path = project_root / config_path
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

def load_model_and_tokenizer(model_path, config):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ LoRA æ¨¡å‹
    if (Path(model_path) / "adapter_config.json").exists():
        print("   æ£€æµ‹åˆ° LoRA é€‚é…å™¨ï¼ŒåŠ è½½åŸºç¡€æ¨¡å‹...")
        base_model_name = config['model']['name']
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16 if config['training']['bf16'] else torch.float16,
        )
        model = PeftModel.from_pretrained(base_model, model_path)
        print("   âœ… LoRA é€‚é…å™¨å·²åŠ è½½")
    else:
        # å®Œæ•´æ¨¡å‹
        bnb_config = None
        if config['quantization'].get('load_in_4bit', False):
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16 if not config['training']['bf16'] else torch.bfloat16,
            )
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=bnb_config,
            device_map="auto",
            torch_dtype=torch.bfloat16 if config['training']['bf16'] else torch.float16,
        )
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    
    return model, tokenizer

def format_prompt(instruction, input_text=""):
    """æ ¼å¼åŒ–æç¤ºè¯ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰"""
    return f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{instruction}\n{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"

def generate_response(model, tokenizer, prompt, max_length=512, temperature=0.7):
    """ç”Ÿæˆå›ç­”"""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            temperature=temperature,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # è§£ç è¾“å‡º
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)
    
    # æå– assistant çš„å›ç­”éƒ¨åˆ†
    if "<|start_header_id|>assistant<|end_header_id|>" in generated_text:
        assistant_response = generated_text.split("<|start_header_id|>assistant<|end_header_id|>")[-1]
        assistant_response = assistant_response.split("<|eot_id|>")[0].strip()
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡è®°ï¼Œè¿”å›ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»é™¤è¾“å…¥ï¼‰
        assistant_response = generated_text[len(prompt):].strip()
    
    return assistant_response

def calculate_perplexity(model, tokenizer, texts):
    """è®¡ç®—å›°æƒ‘åº¦"""
    model.eval()
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for text in texts:
            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512).to(model.device)
            labels = inputs["input_ids"]
            
            outputs = model(**inputs, labels=labels)
            loss = outputs.loss
            total_loss += loss.item() * labels.numel()
            total_tokens += labels.numel()
    
    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')
    perplexity = np.exp(avg_loss)
    return perplexity

def calculate_bleu(references, predictions):
    """è®¡ç®— BLEU åˆ†æ•°"""
    if not BLEU_AVAILABLE:
        return None
    
    smoothing = SmoothingFunction().method1
    bleu_scores = []
    
    for ref, pred in zip(references, predictions):
        ref_tokens = word_tokenize(ref.lower())
        pred_tokens = word_tokenize(pred.lower())
        score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothing)
        bleu_scores.append(score)
    
    return {
        'bleu_1': np.mean(bleu_scores),
        'bleu_avg': np.mean(bleu_scores)
    }

def calculate_rouge(references, predictions):
    """è®¡ç®— ROUGE åˆ†æ•°"""
    if not ROUGE_AVAILABLE:
        return None
    
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}
    
    for ref, pred in zip(references, predictions):
        scores = scorer.score(ref, pred)
        rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)
        rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)
        rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)
    
    return {
        'rouge1': np.mean(rouge_scores['rouge1']),
        'rouge2': np.mean(rouge_scores['rouge2']),
        'rougeL': np.mean(rouge_scores['rougeL'])
    }

def evaluate_model(model, tokenizer, test_dataset, config, max_samples=None):
    """è¯„ä¼°æ¨¡å‹"""
    print(f"\nğŸ“Š å¼€å§‹è¯„ä¼°æ¨¡å‹...")
    print(f"   æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
    if max_samples:
        test_dataset = test_dataset.select(range(min(max_samples, len(test_dataset))))
        print(f"   è¯„ä¼°æ ·æœ¬æ•°: {len(test_dataset)} (é™åˆ¶)")
    
    references = []
    predictions = []
    all_texts = []  # ç”¨äºè®¡ç®—å›°æƒ‘åº¦
    
    print("\nğŸ”„ ç”Ÿæˆé¢„æµ‹...")
    for idx, example in enumerate(test_dataset):
        if (idx + 1) % 100 == 0:
            print(f"   è¿›åº¦: {idx + 1}/{len(test_dataset)}")
        
        instruction = example.get('instruction', '')
        input_text = example.get('input', '')
        reference = example.get('output', '')
        
        # æ ¼å¼åŒ–æç¤ºè¯
        prompt = format_prompt(instruction, input_text)
        
        # ç”Ÿæˆå›ç­”
        prediction = generate_response(model, tokenizer, prompt)
        
        references.append(reference)
        predictions.append(prediction)
        
        # ç”¨äºå›°æƒ‘åº¦è®¡ç®—
        full_text = prompt + reference
        all_texts.append(full_text)
    
    print("\nğŸ“ˆ è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    
    # è®¡ç®—å›°æƒ‘åº¦
    print("   è®¡ç®—å›°æƒ‘åº¦...")
    perplexity = calculate_perplexity(model, tokenizer, all_texts[:100])  # é™åˆ¶æ ·æœ¬æ•°ä»¥åŠ å¿«é€Ÿåº¦
    
    # è®¡ç®— BLEU
    bleu_scores = None
    if BLEU_AVAILABLE:
        print("   è®¡ç®— BLEU åˆ†æ•°...")
        bleu_scores = calculate_bleu(references, predictions)
    
    # è®¡ç®— ROUGE
    rouge_scores = None
    if ROUGE_AVAILABLE:
        print("   è®¡ç®— ROUGE åˆ†æ•°...")
        rouge_scores = calculate_rouge(references, predictions)
    
    # è®¡ç®—å¹³å‡é•¿åº¦
    avg_ref_length = np.mean([len(ref) for ref in references])
    avg_pred_length = np.mean([len(pred) for pred in predictions])
    
    # æ±‡æ€»ç»“æœ
    results = {
        'evaluation_time': datetime.now().isoformat(),
        'num_samples': len(test_dataset),
        'perplexity': float(perplexity) if perplexity else None,
        'bleu': bleu_scores,
        'rouge': rouge_scores,
        'average_lengths': {
            'reference': float(avg_ref_length),
            'prediction': float(avg_pred_length)
        },
        'sample_predictions': [
            {
                'instruction': test_dataset[i]['instruction'][:100] + '...' if len(test_dataset[i]['instruction']) > 100 else test_dataset[i]['instruction'],
                'reference': references[i][:200] + '...' if len(references[i]) > 200 else references[i],
                'prediction': predictions[i][:200] + '...' if len(predictions[i]) > 200 else predictions[i],
            }
            for i in range(min(5, len(test_dataset)))
        ]
    }
    
    return results

def print_results(results):
    """æ‰“å°è¯„ä¼°ç»“æœ"""
    print("\n" + "="*60)
    print("ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ")
    print("="*60)
    print(f"è¯„ä¼°æ—¶é—´: {results['evaluation_time']}")
    print(f"è¯„ä¼°æ ·æœ¬æ•°: {results['num_samples']}")
    print()
    
    if results['perplexity']:
        print(f"å›°æƒ‘åº¦ (Perplexity): {results['perplexity']:.2f}")
    
    if results['bleu']:
        print(f"\nBLEU åˆ†æ•°:")
        print(f"  BLEU-1: {results['bleu']['bleu_1']:.4f}")
        print(f"  BLEU-Avg: {results['bleu']['bleu_avg']:.4f}")
    
    if results['rouge']:
        print(f"\nROUGE åˆ†æ•°:")
        print(f"  ROUGE-1: {results['rouge']['rouge1']:.4f}")
        print(f"  ROUGE-2: {results['rouge']['rouge2']:.4f}")
        print(f"  ROUGE-L: {results['rouge']['rougeL']:.4f}")
    
    print(f"\nå¹³å‡é•¿åº¦:")
    print(f"  å‚è€ƒç­”æ¡ˆ: {results['average_lengths']['reference']:.1f} å­—ç¬¦")
    print(f"  ç”Ÿæˆç­”æ¡ˆ: {results['average_lengths']['prediction']:.1f} å­—ç¬¦")
    
    print(f"\nç¤ºä¾‹é¢„æµ‹ (å‰ 3 ä¸ª):")
    for i, sample in enumerate(results['sample_predictions'][:3], 1):
        print(f"\nç¤ºä¾‹ {i}:")
        print(f"  é—®é¢˜: {sample['instruction']}")
        print(f"  å‚è€ƒç­”æ¡ˆ: {sample['reference']}")
        print(f"  æ¨¡å‹é¢„æµ‹: {sample['prediction']}")

def main():
    parser = argparse.ArgumentParser(description='è¯„ä¼°è®­ç»ƒåçš„æ¨¡å‹')
    parser.add_argument('--model_path', type=str, required=True,
                       help='æ¨¡å‹è·¯å¾„ï¼ˆLoRA é€‚é…å™¨æˆ–å®Œæ•´æ¨¡å‹ï¼‰')
    parser.add_argument('--test_path', type=str, default=None,
                       help='æµ‹è¯•é›†è·¯å¾„ï¼ˆé»˜è®¤: config ä¸­çš„ test_pathï¼‰')
    parser.add_argument('--config', type=str, default=None,
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: config/train_config.yamlï¼‰')
    parser.add_argument('--max_samples', type=int, default=None,
                       help='æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰')
    parser.add_argument('--output', type=str, default=None,
                       help='è¯„ä¼°ç»“æœä¿å­˜è·¯å¾„ï¼ˆé»˜è®¤: model_path/evaluation_results.jsonï¼‰')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # ç¡®å®šæµ‹è¯•é›†è·¯å¾„
    if args.test_path:
        test_path = Path(args.test_path)
    else:
        test_path = project_root / config['data'].get('test_path', 'data/datasets/test.jsonl')
    
    if not test_path.exists():
        print(f"âŒ é”™è¯¯: æµ‹è¯•é›†ä¸å­˜åœ¨: {test_path}")
        return
    
    # ç¡®å®šæ¨¡å‹è·¯å¾„
    model_path = Path(args.model_path)
    if not model_path.is_absolute():
        model_path = project_root / model_path
    
    if not model_path.exists():
        print(f"âŒ é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = load_model_and_tokenizer(str(model_path), config)
    
    # åŠ è½½æµ‹è¯•é›†
    print(f"\nğŸ“‚ åŠ è½½æµ‹è¯•é›†: {test_path}")
    test_dataset = load_dataset("json", data_files=str(test_path), split="train")
    
    # è¯„ä¼°æ¨¡å‹
    results = evaluate_model(model, tokenizer, test_dataset, config, args.max_samples)
    
    # æ‰“å°ç»“æœ
    print_results(results)
    
    # ä¿å­˜ç»“æœ
    if args.output:
        output_path = Path(args.output)
    else:
        output_path = model_path / "evaluation_results.json"
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print("="*60)

if __name__ == "__main__":
    main()

