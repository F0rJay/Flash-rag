import os
from pathlib import Path
# è®¾ç½® HuggingFace é•œåƒç¯å¢ƒå˜é‡ï¼ˆè§£å†³ç½‘ç»œè¿æ¥é—®é¢˜ï¼‰
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# è·å–é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent.parent

# å®šä¹‰å‘é‡åº“è·¯å¾„ï¼ˆæ”¯æŒå¤šä¸ªçŸ¥è¯†åº“ï¼‰
DEFAULT_PERSIST_DIR = str(project_root / "chroma_db")
# å®šä¹‰ç”¨äºåµŒå…¥çš„å¼€æºæ¨¡å‹ï¼ˆéœ€æœ¬åœ°å®‰è£… sentence-transformersï¼‰
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2" # è¿™æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„å¿«é€Ÿæ¨¡å‹

def run_ingestion(docs_path=None, chunk_size=500, chunk_overlap=50, persist_dir=None, knowledge_type="law"):
    """
    è¿è¡Œæ–‡æ¡£å‘é‡åŒ–å¤„ç†
    
    Args:
        docs_path: æ–‡æ¡£è·¯å¾„ï¼ˆé»˜è®¤: data/docs/legal_docs.txtï¼‰
        chunk_size: æ–‡æ¡£å—å¤§å°ï¼ˆé»˜è®¤: 500 å­—ç¬¦ï¼‰
        chunk_overlap: å—ä¹‹é—´é‡å å¤§å°ï¼ˆé»˜è®¤: 50 å­—ç¬¦ï¼‰
        persist_dir: å‘é‡åº“ä¿å­˜è·¯å¾„ï¼ˆé»˜è®¤æ ¹æ® knowledge_type è‡ªåŠ¨ç”Ÿæˆï¼‰
        knowledge_type: çŸ¥è¯†åº“ç±»å‹ ("law"=æ³•æ¡å‹, "case"=æ¡ˆä¾‹å‹, "judgement"=åˆ¤å†³ä¹¦å‹, é»˜è®¤: "law")
    """
    # 1. åŠ è½½æ–‡æ¡£ (Load Documents)
    if docs_path is None:
        docs_path = project_root / "data" / "docs" / "legal_docs.txt"
    else:
        docs_path = Path(docs_path)
        if not docs_path.is_absolute():
            docs_path = project_root / docs_path
    
    if not docs_path.exists():
        print(f"âŒ é”™è¯¯: æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨: {docs_path}")
        print(f"ğŸ’¡ æç¤º: è¯·å…ˆè¿è¡Œ 'python scripts/prepare_rag_knowledge.py' å‡†å¤‡çŸ¥è¯†åº“")
        return None
    
    print(f"ğŸ“‚ åŠ è½½æ–‡æ¡£: {docs_path}")
    loader = TextLoader(str(docs_path), encoding='utf-8')
    documents = loader.load()
    print(f"âœ… åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    
    # 2. æ–‡æ¡£åˆ‡åˆ† (Text Splitting)
    # å¯¹äºæ³•å¾‹æ¡æ–‡ï¼Œé€‚å½“å¢å¤§ chunk_size ä»¥ä¿æŒå®Œæ•´æ€§
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,        # æ¯ä¸ªå—æœ€å¤§å­—ç¬¦æ•°
        chunk_overlap=chunk_overlap,  # å—ä¹‹é—´é‡å å­—ç¬¦æ•°ï¼Œä¿æŒä¸Šä¸‹æ–‡
        separators=["\n\n", "\n", "ã€‚", "ï¼›", "ï¼Œ", " ", ""]  # ä¼˜å…ˆæŒ‰æ®µè½åˆ†å‰²
    )
    texts = text_splitter.split_documents(documents)
    print(f"âœ… åˆ‡åˆ†ä¸º {len(texts)} ä¸ªæ–‡æ¡£å—")

    # 3. åˆ›å»ºåµŒå…¥æ¨¡å‹ (Create Embeddings)
    # è¿™å°†è´Ÿè´£å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡
    print(f"ğŸ”„ åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {EMBEDDING_MODEL_NAME}")
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    
    # 4. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ (Store in VectorDB)
    # è¿™æ˜¯åˆ›å»º RAG çŸ¥è¯†åº“çš„æ ¸å¿ƒæ­¥éª¤
    
    # ç¡®å®šå‘é‡åº“ä¿å­˜è·¯å¾„
    if persist_dir is None:
        if knowledge_type == "case":
            persist_dir = str(project_root / "chroma_db_case")
        elif knowledge_type == "judgement":
            persist_dir = str(project_root / "chroma_db_judgement")
        else:
            persist_dir = DEFAULT_PERSIST_DIR
    else:
        persist_dir = str(Path(persist_dir).resolve())
    
    print(f"ğŸ’¾ æ„å»ºå‘é‡æ•°æ®åº“...")
    print(f"ğŸ“ ä¿å­˜è·¯å¾„: {persist_dir}")
    vectordb = Chroma.from_documents(
        documents=texts,
        embedding=embeddings,
        persist_directory=persist_dir
    )
    # æ³¨æ„ï¼šæ–°ç‰ˆæœ¬çš„ Chroma åœ¨ä½¿ç”¨ persist_directory æ—¶ä¼šè‡ªåŠ¨æŒä¹…åŒ–ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒç”¨ persist()
    print(f"âœ… å‘é‡åŒ–å®Œæˆï¼çŸ¥è¯†åº“å·²ä¿å­˜åˆ°: {persist_dir}")
    print(f"ğŸ“Š ç»Ÿè®¡: {len(texts)} ä¸ªæ–‡æ¡£å—å·²å‘é‡åŒ–")
    return vectordb

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æ–‡æ¡£å‘é‡åŒ–å¤„ç†ï¼ˆæ„å»º RAG çŸ¥è¯†åº“ï¼‰')
    parser.add_argument('--docs-path', type=str, default=None,
                       help='æ–‡æ¡£æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: data/docs/legal_docs.txtï¼‰')
    parser.add_argument('--chunk-size', type=int, default=500,
                       help='æ–‡æ¡£å—å¤§å°ï¼ˆé»˜è®¤: 500 å­—ç¬¦ï¼‰')
    parser.add_argument('--chunk-overlap', type=int, default=50,
                       help='å—ä¹‹é—´é‡å å¤§å°ï¼ˆé»˜è®¤: 50 å­—ç¬¦ï¼‰')
    parser.add_argument('--persist-dir', type=str, default=None,
                       help='å‘é‡åº“ä¿å­˜è·¯å¾„ï¼ˆé»˜è®¤æ ¹æ®çŸ¥è¯†åº“ç±»å‹è‡ªåŠ¨ç”Ÿæˆï¼‰')
    parser.add_argument('--knowledge-type', type=str, choices=['law', 'case', 'judgement'], default='law',
                       help='çŸ¥è¯†åº“ç±»å‹: law=æ³•æ¡å‹, case=æ¡ˆä¾‹å‹, judgement=åˆ¤å†³ä¹¦å‹ï¼ˆé»˜è®¤: lawï¼‰')
    
    args = parser.parse_args()
    
    # æ ¹æ®çŸ¥è¯†åº“ç±»å‹è®¾ç½®é»˜è®¤æ–‡æ¡£è·¯å¾„
    if args.docs_path is None:
        if args.knowledge_type == "case":
            args.docs_path = project_root / "data" / "docs" / "case_docs.txt"
        elif args.knowledge_type == "judgement":
            args.docs_path = project_root / "data" / "docs" / "judgement_docs.txt"
        else:
            args.docs_path = project_root / "data" / "docs" / "legal_docs.txt"
    
    run_ingestion(
        docs_path=args.docs_path,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
        persist_dir=args.persist_dir,
        knowledge_type=args.knowledge_type
    )