#!/usr/bin/env python3
"""
æ•°æ®é›†å‡†å¤‡è„šæœ¬
åŠŸèƒ½ï¼š
1. å°† DISC-Law æ ¼å¼è½¬æ¢ä¸ºé¡¹ç›®éœ€è¦çš„æ ¼å¼
2. å°†æ•°æ®é›†åˆ†æˆè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
3. ä¿å­˜åˆ° data/datasets/ ç›®å½•

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/prepare_dataset.py <input_file> [--train-ratio 0.8] [--val-ratio 0.1] [--test-ratio 0.1]
"""

import json
import argparse
from pathlib import Path
from typing import Dict, List
import random

def load_jsonl(file_path: Path) -> List[Dict]:
    """åŠ è½½ JSONL æ–‡ä»¶"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data

def convert_format(item: Dict) -> Dict:
    """
    è½¬æ¢æ•°æ®æ ¼å¼
    ä» DISC-Law æ ¼å¼: {"id": "...", "input": "...", "output": "..."}
    è½¬æ¢ä¸ºé¡¹ç›®æ ¼å¼: {"instruction": "...", "input": "...", "output": "..."}
    """
    # DISC-Law æ ¼å¼ï¼šinput æ˜¯é—®é¢˜ï¼Œoutput æ˜¯ç­”æ¡ˆ
    # é¡¹ç›®æ ¼å¼ï¼šinstruction æ˜¯é—®é¢˜ï¼Œinput æ˜¯ä¸Šä¸‹æ–‡ï¼ˆå¯ä¸ºç©ºï¼‰ï¼Œoutput æ˜¯ç­”æ¡ˆ
    
    converted = {
        "instruction": item.get("input", ""),  # é—®é¢˜ä½œä¸º instruction
        "input": "",  # é€šå¸¸ä¸ºç©ºï¼Œå¦‚æœæœ‰ä¸Šä¸‹æ–‡å¯ä»¥å¡«å……
        "output": item.get("output", "")  # ç­”æ¡ˆ
    }
    
    return converted

def split_dataset(data: List[Dict], train_ratio: float, val_ratio: float, test_ratio: float) -> tuple:
    """
    åˆ’åˆ†æ•°æ®é›†
    è¿”å›: (train_data, val_data, test_data)
    """
    # éªŒè¯æ¯”ä¾‹
    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ç­‰äº 1.0"
    
    # æ‰“ä¹±æ•°æ®
    random.seed(42)  # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°
    shuffled_data = data.copy()
    random.shuffle(shuffled_data)
    
    total = len(shuffled_data)
    train_end = int(total * train_ratio)
    val_end = train_end + int(total * val_ratio)
    
    train_data = shuffled_data[:train_end]
    val_data = shuffled_data[train_end:val_end]
    test_data = shuffled_data[val_end:]
    
    return train_data, val_data, test_data

def save_jsonl(data: List[Dict], file_path: Path):
    """ä¿å­˜ä¸º JSONL æ ¼å¼"""
    with open(file_path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

def main():
    parser = argparse.ArgumentParser(description='å‡†å¤‡è®­ç»ƒæ•°æ®é›†')
    parser.add_argument('input_file', type=str, nargs='?', help='è¾“å…¥çš„ JSONL æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœå·²æœ‰ train/val/test æ–‡ä»¶åˆ™ä¸éœ€è¦ï¼‰')
    parser.add_argument('--train-ratio', type=float, default=0.8, help='è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤: 0.8ï¼‰')
    parser.add_argument('--val-ratio', type=float, default=0.1, help='éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤: 0.1ï¼‰')
    parser.add_argument('--test-ratio', type=float, default=0.1, help='æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆé»˜è®¤: 0.1ï¼‰')
    parser.add_argument('--use-existing', action='store_true', help='ç›´æ¥ä½¿ç”¨å·²æœ‰çš„ train/val/test.jsonl æ–‡ä»¶ï¼Œä¸è¿›è¡Œè½¬æ¢å’Œåˆ’åˆ†')
    parser.add_argument('--validate', action='store_true', help='éªŒè¯å·²æœ‰æ•°æ®é›†æ ¼å¼')
    
    args = parser.parse_args()
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    output_dir = project_root / "data" / "datasets"
    
    # å¦‚æœä½¿ç”¨å·²æœ‰æ•°æ®é›†
    if args.use_existing or args.validate:
        train_file = output_dir / "train.jsonl"
        val_file = output_dir / "val.jsonl"
        test_file = output_dir / "test.jsonl"
        
        existing_files = []
        for name, file_path in [("è®­ç»ƒé›†", train_file), ("éªŒè¯é›†", val_file), ("æµ‹è¯•é›†", test_file)]:
            if file_path.exists():
                existing_files.append((name, file_path))
                print(f"âœ… {name}å­˜åœ¨: {file_path}")
            else:
                print(f"âš ï¸  {name}ä¸å­˜åœ¨: {file_path}")
        
        if args.validate:
            # éªŒè¯æ•°æ®é›†
            print("\nğŸ” éªŒè¯æ•°æ®é›†æ ¼å¼...")
            import json
            all_valid = True
            for name, file_path in existing_files:
                data = []
                errors = []
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        if not line.strip():
                            continue
                        try:
                            item = json.loads(line)
                            data.append(item)
                            # ç®€å•éªŒè¯
                            if 'instruction' not in item or 'output' not in item:
                                errors.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ")
                            elif not item.get('instruction', '').strip() or not item.get('output', '').strip():
                                errors.append(f"å­—æ®µä¸ºç©º")
                        except json.JSONDecodeError as e:
                            errors.append(f"JSONè§£æé”™è¯¯: {e}")
                
                if errors:
                    print(f"âŒ {name} æœ‰ {len(errors)} ä¸ªé”™è¯¯")
                    all_valid = False
                else:
                    print(f"âœ… {name} æ ¼å¼æ­£ç¡®ï¼Œå…± {len(data)} æ¡æ•°æ®")
            
            if all_valid:
                print("\nâœ… æ‰€æœ‰æ•°æ®é›†éªŒè¯é€šè¿‡ï¼")
            return
        
        if args.use_existing:
            print(f"\nâœ… ä½¿ç”¨å·²æœ‰æ•°æ®é›†ï¼Œæ— éœ€è½¬æ¢")
            return
    
    # å¦‚æœæ²¡æœ‰æä¾›è¾“å…¥æ–‡ä»¶ï¼Œæç¤ºç”¨æˆ·
    if not args.input_file:
        print("âŒ é”™è¯¯: è¯·æä¾›è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæˆ–ä½¿ç”¨ --use-existing ä½¿ç”¨å·²æœ‰æ•°æ®é›†")
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  1. ä»æ–°æ–‡ä»¶è½¬æ¢: python scripts/prepare_dataset.py <input_file>")
        print("  2. ä½¿ç”¨å·²æœ‰æ•°æ®é›†: python scripts/prepare_dataset.py --use-existing")
        print("  3. éªŒè¯æ•°æ®é›†: python scripts/prepare_dataset.py --validate")
        return
    
    # è¾“å…¥æ–‡ä»¶è·¯å¾„
    input_file = Path(args.input_file)
    if not input_file.is_absolute():
        # å°è¯•ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„çˆ¶ç›®å½•ï¼ˆautodl-tmpï¼‰
        if (project_root.parent / input_file).exists():
            input_file = project_root.parent / input_file
        # æˆ–è€…ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•
        elif Path(input_file).exists():
            input_file = Path(input_file).resolve()
        else:
            input_file = project_root / input_file
    
    if not input_file.exists():
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return
    
    print(f"ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶: {input_file}")
    
    # åŠ è½½æ•°æ®
    raw_data = load_jsonl(input_file)
    print(f"âœ… åŠ è½½äº† {len(raw_data)} æ¡æ•°æ®")
    
    # è½¬æ¢æ ¼å¼
    print("ğŸ”„ è½¬æ¢æ•°æ®æ ¼å¼...")
    converted_data = [convert_format(item) for item in raw_data]
    print(f"âœ… è½¬æ¢å®Œæˆ")
    
    # åˆ’åˆ†æ•°æ®é›†
    print(f"ğŸ“Š åˆ’åˆ†æ•°æ®é›† (è®­ç»ƒ:{args.train_ratio}, éªŒè¯:{args.val_ratio}, æµ‹è¯•:{args.test_ratio})...")
    train_data, val_data, test_data = split_dataset(
        converted_data, 
        args.train_ratio, 
        args.val_ratio, 
        args.test_ratio
    )
    
    print(f"âœ… åˆ’åˆ†å®Œæˆ:")
    print(f"   - è®­ç»ƒé›†: {len(train_data)} æ¡")
    print(f"   - éªŒè¯é›†: {len(val_data)} æ¡")
    print(f"   - æµ‹è¯•é›†: {len(test_data)} æ¡")
    
    # ä¿å­˜æ–‡ä»¶
    output_dir.mkdir(parents=True, exist_ok=True)
    
    train_file = output_dir / "train.jsonl"
    val_file = output_dir / "val.jsonl"
    test_file = output_dir / "test.jsonl"
    
    print(f"ğŸ’¾ ä¿å­˜æ–‡ä»¶...")
    save_jsonl(train_data, train_file)
    save_jsonl(val_data, val_file)
    save_jsonl(test_data, test_file)
    
    print(f"âœ… ä¿å­˜å®Œæˆ:")
    print(f"   - è®­ç»ƒé›†: {train_file}")
    print(f"   - éªŒè¯é›†: {val_file}")
    print(f"   - æµ‹è¯•é›†: {test_file}")
    
    # æ˜¾ç¤ºç¤ºä¾‹
    print("\nğŸ“ æ•°æ®æ ¼å¼ç¤ºä¾‹ï¼ˆè®­ç»ƒé›†ç¬¬ä¸€æ¡ï¼‰:")
    print(json.dumps(train_data[0], ensure_ascii=False, indent=2))

if __name__ == "__main__":
    main()

