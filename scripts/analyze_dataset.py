#!/usr/bin/env python3
"""
æ•°æ®é›†åˆ†æå’ŒéªŒè¯è„šæœ¬
åŠŸèƒ½ï¼š
1. éªŒè¯æ•°æ®é›†æ ¼å¼æ˜¯å¦æ­£ç¡®
2. ç»Ÿè®¡æ•°æ®é›†åŸºæœ¬ä¿¡æ¯ï¼ˆæ•°é‡ã€é•¿åº¦åˆ†å¸ƒç­‰ï¼‰
3. æ£€æŸ¥æ•°æ®è´¨é‡ï¼ˆç©ºå€¼ã€é‡å¤ç­‰ï¼‰
4. ç”Ÿæˆæ•°æ®é›†æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/analyze_dataset.py [--train data/datasets/train.jsonl] [--val data/datasets/val.jsonl] [--test data/datasets/test.jsonl]
"""

import json
import argparse
from pathlib import Path
from typing import Dict, List, Optional
from collections import Counter
import statistics

def load_jsonl(file_path: Path) -> List[Dict]:
    """åŠ è½½ JSONL æ–‡ä»¶"""
    data = []
    errors = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            if not line.strip():
                continue
            try:
                data.append(json.loads(line))
            except json.JSONDecodeError as e:
                errors.append((line_num, str(e)))
    return data, errors

def validate_format(item: Dict, required_fields: List[str] = None) -> tuple:
    """
    éªŒè¯æ•°æ®æ ¼å¼
    è¿”å›: (is_valid, error_message)
    """
    if required_fields is None:
        required_fields = ['instruction', 'input', 'output']
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    for field in required_fields:
        if field not in item:
            return False, f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"
        if not isinstance(item[field], str):
            return False, f"å­—æ®µ {field} å¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹"
    
    # æ£€æŸ¥ instruction å’Œ output ä¸èƒ½ä¸ºç©º
    if not item['instruction'].strip():
        return False, "instruction å­—æ®µä¸èƒ½ä¸ºç©º"
    if not item['output'].strip():
        return False, "output å­—æ®µä¸èƒ½ä¸ºç©º"
    
    return True, ""

def analyze_dataset(data: List[Dict], dataset_name: str) -> Dict:
    """åˆ†ææ•°æ®é›†"""
    stats = {
        'name': dataset_name,
        'total': len(data),
        'valid': 0,
        'invalid': 0,
        'errors': [],
        'length_stats': {},
        'field_stats': {}
    }
    
    # ç»Ÿè®¡å­—æ®µé•¿åº¦
    instruction_lengths = []
    input_lengths = []
    output_lengths = []
    total_lengths = []
    
    # éªŒè¯æ•°æ®
    for idx, item in enumerate(data):
        is_valid, error_msg = validate_format(item)
        if is_valid:
            stats['valid'] += 1
            # ç»Ÿè®¡é•¿åº¦
            inst_len = len(item['instruction'])
            input_len = len(item.get('input', ''))
            output_len = len(item['output'])
            total_len = inst_len + input_len + output_len
            
            instruction_lengths.append(inst_len)
            input_lengths.append(input_len)
            output_lengths.append(output_len)
            total_lengths.append(total_len)
        else:
            stats['invalid'] += 1
            stats['errors'].append({
                'index': idx,
                'error': error_msg,
                'item': {k: v[:100] if isinstance(v, str) and len(v) > 100 else v 
                        for k, v in item.items()}
            })
    
    # è®¡ç®—é•¿åº¦ç»Ÿè®¡
    if instruction_lengths:
        stats['length_stats']['instruction'] = {
            'min': min(instruction_lengths),
            'max': max(instruction_lengths),
            'mean': round(statistics.mean(instruction_lengths), 2),
            'median': statistics.median(instruction_lengths)
        }
    
    if input_lengths:
        stats['length_stats']['input'] = {
            'min': min(input_lengths),
            'max': max(input_lengths),
            'mean': round(statistics.mean(input_lengths), 2),
            'median': statistics.median(input_lengths)
        }
    
    if output_lengths:
        stats['length_stats']['output'] = {
            'min': min(output_lengths),
            'max': max(output_lengths),
            'mean': round(statistics.mean(output_lengths), 2),
            'median': statistics.median(output_lengths)
        }
    
    if total_lengths:
        stats['length_stats']['total'] = {
            'min': min(total_lengths),
            'max': max(total_lengths),
            'mean': round(statistics.mean(total_lengths), 2),
            'median': statistics.median(total_lengths)
        }
    
    # æ£€æŸ¥é‡å¤
    unique_instructions = set()
    duplicates = []
    for idx, item in enumerate(data):
        inst = item.get('instruction', '').strip()
        if inst in unique_instructions:
            duplicates.append(idx)
        else:
            unique_instructions.add(inst)
    
    stats['duplicates'] = len(duplicates)
    stats['unique_instructions'] = len(unique_instructions)
    
    return stats

def print_stats(stats: Dict):
    """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æ•°æ®é›†åˆ†æ: {stats['name']}")
    print(f"{'='*60}")
    print(f"æ€»æ•°é‡: {stats['total']}")
    print(f"âœ… æœ‰æ•ˆ: {stats['valid']}")
    print(f"âŒ æ— æ•ˆ: {stats['invalid']}")
    print(f"ğŸ”„ é‡å¤æŒ‡ä»¤: {stats['duplicates']}")
    print(f"âœ¨ å”¯ä¸€æŒ‡ä»¤: {stats['unique_instructions']}")
    
    if stats['length_stats']:
        print(f"\nğŸ“ é•¿åº¦ç»Ÿè®¡:")
        for field, lengths in stats['length_stats'].items():
            print(f"  {field}:")
            print(f"    æœ€å°: {lengths['min']}")
            print(f"    æœ€å¤§: {lengths['max']}")
            print(f"    å¹³å‡: {lengths['mean']}")
            print(f"    ä¸­ä½æ•°: {lengths['median']}")
    
    if stats['errors']:
        print(f"\nâš ï¸  é”™è¯¯ç¤ºä¾‹ (å‰ 5 ä¸ª):")
        for error in stats['errors'][:5]:
            print(f"  ç´¢å¼• {error['index']}: {error['error']}")
            print(f"    æ•°æ®: {error['item']}")

def main():
    parser = argparse.ArgumentParser(description='åˆ†æå’ŒéªŒè¯æ•°æ®é›†')
    parser.add_argument('--train', type=str, default='data/datasets/train.jsonl',
                       help='è®­ç»ƒé›†è·¯å¾„ï¼ˆé»˜è®¤: data/datasets/train.jsonlï¼‰')
    parser.add_argument('--val', type=str, default='data/datasets/val.jsonl',
                       help='éªŒè¯é›†è·¯å¾„ï¼ˆé»˜è®¤: data/datasets/val.jsonlï¼‰')
    parser.add_argument('--test', type=str, default='data/datasets/test.jsonl',
                       help='æµ‹è¯•é›†è·¯å¾„ï¼ˆé»˜è®¤: data/datasets/test.jsonlï¼‰')
    parser.add_argument('--output', type=str, default=None,
                       help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼ŒJSON æ ¼å¼ï¼‰')
    
    args = parser.parse_args()
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    
    all_stats = {}
    
    # åˆ†ææ¯ä¸ªæ•°æ®é›†
    datasets = [
        ('train', args.train),
        ('val', args.val),
        ('test', args.test)
    ]
    
    for name, file_path in datasets:
        file_path = Path(file_path)
        if not file_path.is_absolute():
            file_path = project_root / file_path
        
        if not file_path.exists():
            print(f"âš ï¸  è­¦å‘Š: {name} æ•°æ®é›†ä¸å­˜åœ¨: {file_path}")
            continue
        
        print(f"\nğŸ“‚ åŠ è½½ {name} æ•°æ®é›†: {file_path}")
        data, errors = load_jsonl(file_path)
        
        if errors:
            print(f"âš ï¸  è­¦å‘Š: å‘ç° {len(errors)} ä¸ª JSON è§£æé”™è¯¯")
            for line_num, error in errors[:3]:
                print(f"  ç¬¬ {line_num} è¡Œ: {error}")
        
        stats = analyze_dataset(data, name)
        all_stats[name] = stats
        print_stats(stats)
    
    # æ±‡æ€»ç»Ÿè®¡
    if len(all_stats) > 1:
        print(f"\n{'='*60}")
        print(f"ğŸ“ˆ æ±‡æ€»ç»Ÿè®¡")
        print(f"{'='*60}")
        total_samples = sum(s['total'] for s in all_stats.values())
        total_valid = sum(s['valid'] for s in all_stats.values())
        total_invalid = sum(s['invalid'] for s in all_stats.values())
        
        print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"æ€»æœ‰æ•ˆæ•°: {total_valid}")
        print(f"æ€»æ— æ•ˆæ•°: {total_invalid}")
        print(f"æœ‰æ•ˆç‡: {total_valid/total_samples*100:.2f}%")
        
        print(f"\nå„æ•°æ®é›†åˆ†å¸ƒ:")
        for name, stats in all_stats.items():
            ratio = stats['total'] / total_samples * 100 if total_samples > 0 else 0
            print(f"  {name}: {stats['total']} ({ratio:.1f}%)")
    
    # ä¿å­˜æŠ¥å‘Š
    if args.output:
        output_path = Path(args.output)
        if not output_path.is_absolute():
            output_path = project_root / output_path
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(all_stats, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")

if __name__ == "__main__":
    main()

