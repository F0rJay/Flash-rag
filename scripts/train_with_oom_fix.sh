#!/bin/bash
# è®­ç»ƒè„šæœ¬ï¼ˆå¸¦ OOM ä¿®å¤ï¼‰
# è®¾ç½® PyTorch CUDA å†…å­˜åˆ†é…ä¼˜åŒ–

set -e

echo "ğŸš€ å¯åŠ¨è®­ç»ƒï¼ˆå·²ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ï¼‰"
echo ""

# è®¾ç½® PyTorch CUDA å†…å­˜åˆ†é…ä¼˜åŒ–ï¼ˆå‡å°‘å†…å­˜ç¢ç‰‡ï¼‰
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

echo "ğŸ“ æ˜¾å­˜ä¼˜åŒ–è®¾ç½®:"
echo "  - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
echo ""

# æ£€æŸ¥é…ç½®
echo "ğŸ“‹ å½“å‰è®­ç»ƒé…ç½®:"
echo "  - æ‰¹æ¬¡å¤§å°: $(grep 'per_device_train_batch_size' config/train_config.yaml | awk '{print $2}')"
echo "  - æ¢¯åº¦ç´¯ç§¯: $(grep 'gradient_accumulation_steps' config/train_config.yaml | awk '{print $2}')"
echo "  - åºåˆ—é•¿åº¦: $(grep 'max_seq_length' config/train_config.yaml | awk '{print $2}')"
echo "  - LoRA rank: $(grep '^  r:' config/train_config.yaml | awk '{print $2}')"
echo "  - æ¢¯åº¦æ£€æŸ¥ç‚¹: $(grep 'gradient_checkpointing' config/train_config.yaml | awk '{print $2}')"
echo ""

# è¿è¡Œè®­ç»ƒ
cd "$(dirname "$0")/.."
python train.py

