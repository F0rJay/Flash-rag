## Flash-RAG

Flash-RAG æ˜¯ä¸€ä¸ªåŸºäº **vLLM** çš„é«˜å¹¶å‘å‚ç›´é¢†åŸŸæ™ºèƒ½é—®ç­”å¼•æ“ï¼Œå½“å‰ä¸»è¦èšç„¦äº **æ³•å¾‹æ¡æ–‡å’¨è¯¢åŠ©æ‰‹** åœºæ™¯ã€‚

---

## é¡¹ç›®ç»“æ„

```text
Flash-RAG/
â”œâ”€â”€ src/                   # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ core/              # æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ CustomVLLM.py  # è‡ªå®šä¹‰ vLLM é›†æˆ
â”‚   â”‚   â””â”€â”€ ingest.py      # æ–‡æ¡£å‘é‡åŒ–å¤„ç†
â”‚   â”œâ”€â”€ api/               # API æœåŠ¡
â”‚   â”‚   â””â”€â”€ main.py       # FastAPI RAG æœåŠ¡
â”‚   â”œâ”€â”€ training/          # è®­ç»ƒç›¸å…³
â”‚   â”‚   â”œâ”€â”€ train.py      # æ¨¡å‹è®­ç»ƒè„šæœ¬
â”‚   â”‚   â””â”€â”€ merge.py      # æƒé‡åˆå¹¶è„šæœ¬
â”‚   â””â”€â”€ frontend/         # å‰ç«¯ç›¸å…³
â”‚       â””â”€â”€ frontend.py
â”œâ”€â”€ scripts/              # è„šæœ¬ç›®å½•
â”‚   â”œâ”€â”€ vllm.sh           # vLLM æœåŠ¡å¯åŠ¨è„šæœ¬
â”‚   â”œâ”€â”€ fastapi.sh        # FastAPI æœåŠ¡å¯åŠ¨è„šæœ¬
â”‚   â”œâ”€â”€ check_vllm.sh     # vLLM æœåŠ¡æ£€æŸ¥è„šæœ¬
â”‚   â””â”€â”€ frontend.sh       # å‰ç«¯å¯åŠ¨è„šæœ¬
â”œâ”€â”€ config/               # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â””â”€â”€ train_config.yaml # è®­ç»ƒä¸æ¨¡å‹ç›¸å…³çš„å…¨éƒ¨å‚æ•°
â”œâ”€â”€ data/                 # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ datasets/         # è®­ç»ƒ/è¯„æµ‹æ•°æ®
â”‚   â”‚   â””â”€â”€ train.jsonl
â”‚   â””â”€â”€ docs/             # æ–‡æ¡£æ•°æ®
â”‚       â””â”€â”€ legal_docs.txt
â”œâ”€â”€ tests/                # æµ‹è¯•æ–‡ä»¶
â”‚   â””â”€â”€ test_client.py    # API æµ‹è¯•å®¢æˆ·ç«¯
â”œâ”€â”€ output/               # è®­ç»ƒè¾“å‡ºä¸æ—¥å¿—ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼Œå·²åŠ å…¥ .gitignoreï¼‰
â”œâ”€â”€ chroma_db/            # å‘é‡æ•°æ®åº“ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼Œå·²åŠ å…¥ .gitignoreï¼‰
â”œâ”€â”€ requirements.txt      # é¡¹ç›®ä¾èµ–
â”œâ”€â”€ .gitignore           # Git å¿½ç•¥è§„åˆ™
â””â”€â”€ README.md            # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

---

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/F0rJay/Flash-rag.git
cd Flash-rag

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. å‡†å¤‡æ•°æ®

ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨ï¼š
- `data/datasets/train.jsonl` - è®­ç»ƒæ•°æ®ï¼ˆæ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ª JSONï¼ŒåŒ…å« `instruction`, `input`, `output` å­—æ®µï¼‰
- `data/docs/legal_docs.txt` - çŸ¥è¯†åº“æ–‡æ¡£ï¼ˆç”¨äº RAGï¼‰

### 3. æ¨¡å‹è®­ç»ƒä¸éƒ¨ç½²

#### æ­¥éª¤ 1: è®­ç»ƒ LoRA é€‚é…å™¨

```bash
# ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
python src/training/train.py
```

è®­ç»ƒé…ç½®åœ¨ `config/train_config.yaml` ä¸­ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼š
- æ¨¡å‹è·¯å¾„
- è®­ç»ƒå‚æ•°ï¼ˆå­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ç­‰ï¼‰
- LoRA å‚æ•°ï¼ˆrankã€alpha ç­‰ï¼‰

#### æ­¥éª¤ 2: åˆå¹¶æƒé‡ï¼ˆå¿…é¡»ï¼ï¼‰

```bash
python src/training/merge.py
```

åˆå¹¶åçš„æ¨¡å‹å°†ä¿å­˜åœ¨ `output/llama3-law-merged/` ç›®å½•ã€‚

#### æ­¥éª¤ 3: æ–‡æ¡£å‘é‡åŒ–ï¼ˆRAG çŸ¥è¯†åº“æ„å»ºï¼‰

```bash
python src/core/ingest.py
```

è¿™å°†ï¼š
- åŠ è½½ `data/docs/legal_docs.txt`
- åˆ‡åˆ†æ–‡æ¡£ä¸ºå—
- ç”Ÿæˆå‘é‡åµŒå…¥
- å­˜å‚¨åˆ° `chroma_db/` å‘é‡æ•°æ®åº“

### 4. å¯åŠ¨æœåŠ¡

#### å¯åŠ¨ vLLM æ¨ç†æœåŠ¡ï¼ˆç»ˆç«¯ 1ï¼‰

```bash
bash scripts/vllm.sh
```

æœåŠ¡å°†åœ¨ `http://localhost:8000` å¯åŠ¨ã€‚

**æ£€æŸ¥æœåŠ¡çŠ¶æ€ï¼š**
```bash
bash scripts/check_vllm.sh
```

#### å¯åŠ¨ FastAPI RAG æœåŠ¡ï¼ˆç»ˆç«¯ 2ï¼‰

```bash
bash scripts/fastapi.sh
```

æœåŠ¡å°†åœ¨ `http://localhost:8080` å¯åŠ¨ã€‚

### 5. æµ‹è¯• API

```bash
# ä½¿ç”¨æµ‹è¯•å®¢æˆ·ç«¯
python tests/test_client.py

# æˆ–ä½¿ç”¨ curl
curl -X POST http://localhost:8080/api/rag/chat \
  -H "Content-Type: application/json" \
  -d '{"query": "å¦‚æœç”²æ–¹é€¾æœŸæ”¯ä»˜æœ¬é‡‘ï¼Œéœ€è¦æ‰¿æ‹…ä»€ä¹ˆè¿çº¦è´£ä»»ï¼Ÿ"}'
```

### é…ç½®è¯´æ˜

æ‰€æœ‰é…ç½®éƒ½åœ¨ `config/train_config.yaml` ä¸­ï¼ŒåŒ…æ‹¬ï¼š
- æ¨¡å‹é…ç½®ï¼ˆæ¨¡å‹åç§°ã€æœ€å¤§åºåˆ—é•¿åº¦ï¼‰
- æ•°æ®é…ç½®ï¼ˆè®­ç»ƒæ•°æ®è·¯å¾„ï¼‰
- è®­ç»ƒå‚æ•°ï¼ˆå­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ã€è®­ç»ƒè½®æ•°ï¼‰
- LoRA å‚æ•°ï¼ˆrankã€alphaã€dropoutï¼‰
- é‡åŒ–é…ç½®ï¼ˆæ˜¯å¦å¯ç”¨ 4-bit é‡åŒ–ï¼‰

---

## ğŸš€ é¡¹ç›®å¼€å‘è¦ç‚¹é€ŸæŸ¥å¡

### æ ¸å¿ƒç›®æ ‡

æ‰“é€ ä¸€ä¸ª **ä½å»¶è¿Ÿã€é«˜å¹¶å‘ã€æ‡‚å‚ç›´é¢†åŸŸçŸ¥è¯†** çš„ç”Ÿäº§çº§ AI é—®ç­”ç³»ç»Ÿã€‚

---

### Phase 1: æ¨¡å‹ç‰¹è®­ (Training & Optimization)

**ä»»åŠ¡ï¼š** è®©æ¨¡å‹"æ‡‚è¡Œ"ä¸”"è½»é‡"ã€‚

**æŠ€æœ¯æ ˆï¼š** HuggingFace Transformers, PEFT, AutoGPTQ / BitsAndBytes

#### å…³é”®æ¦‚å¿µ

| æ¦‚å¿µ | è¯´æ˜ | å…³é”®å‚æ•° |
|------|------|----------|
| **LoRA (Low-Rank Adaptation)** | åªè®­ç»ƒæ—è·¯å°çŸ©é˜µï¼Œå¤§å¹…å‡å°‘è®­ç»ƒæˆæœ¬ | `r` (Rank, å¦‚ 8 æˆ– 16)<br>`target_modules` (é€šå¸¸æ¶µç›–æ‰€æœ‰ Linear layers) |
| **Merge Weights (æƒé‡åˆå¹¶)** | âš ï¸ **å¿…åšæ­¥éª¤ï¼** è®­ç»ƒå®Œå¿…é¡»å°† LoRA æƒé‡åˆå¹¶å›åº•åº§æ¨¡å‹ | ä¿å­˜ä¸ºç‹¬ç«‹çš„ `.safetensors` æ ¼å¼ |
| **Quantization (é‡åŒ–)** | æ¨è AWQ æ ¼å¼ï¼ˆæ¯” GPTQ å¯¹ vLLM æ”¯æŒæ›´å¥½ï¼‰ | å°†æ˜¾å­˜éœ€æ±‚ç åˆ° 1/3 |

#### âš ï¸ é¿å‘æŒ‡å—

> **é‡è¦ï¼š** åªæœ‰åˆå¹¶äº†æƒé‡ï¼Œæ¨ç†é€Ÿåº¦æ‰ä¼šå¿«ã€‚æŒ‚è½½ Adapter æ¨ç†åè€Œä¼šå˜æ…¢ã€‚

**è®­ç»ƒæµç¨‹ï¼š**
```bash
# 1. è®­ç»ƒ LoRA é€‚é…å™¨
python src/training/train.py

# 2. åˆå¹¶æƒé‡ï¼ˆå¿…é¡»ï¼ï¼‰
python src/training/merge.py

# 3. é‡åŒ–ï¼ˆå¯é€‰ï¼Œä½†æ¨èï¼‰
# ä½¿ç”¨ AutoGPTQ æˆ– AWQ å·¥å…·è¿›è¡Œé‡åŒ–
# é‡åŒ–åçš„æ¨¡å‹è·¯å¾„éœ€è¦åœ¨ vllm.sh ä¸­æŒ‡å®š
```

---

### Phase 2: æé€Ÿæ¨ç† (Inference Engine)

**ä»»åŠ¡ï¼š** æ¦¨å¹² GPU æ€§èƒ½ï¼Œè§£å†³æ˜¾å­˜ç“¶é¢ˆã€‚

**æŠ€æœ¯æ ˆï¼š** vLLM

#### æ ¸å¿ƒæœºåˆ¶

- **PagedAttention**: æ˜¾å­˜åˆ†é¡µç®¡ç†ï¼Œæ‹’ç»ç¢ç‰‡åŒ–

#### å¯åŠ¨å‚æ•°ç¤ºä¾‹

ä½¿ç”¨é¡¹ç›®æä¾›çš„è„šæœ¬ï¼ˆæ¨èï¼‰ï¼š
```bash
bash scripts/vllm.sh
```

è„šæœ¬ä¼šè‡ªåŠ¨ï¼š
- æ£€æµ‹æ¨¡å‹è·¯å¾„ï¼ˆ`output/llama3-law-merged`ï¼‰
- è®¾ç½®åˆé€‚çš„æ˜¾å­˜ä½¿ç”¨ç‡ï¼ˆ0.85ï¼‰
- é…ç½®å¹¶å‘é™åˆ¶ï¼ˆmax-num-seqs 128ï¼‰

æ‰‹åŠ¨å¯åŠ¨ï¼ˆå¦‚éœ€è‡ªå®šä¹‰å‚æ•°ï¼‰ï¼š
```bash
vllm serve \
    output/llama3-law-merged \
    --host 0.0.0.0 \
    --port 8000 \
    --dtype bfloat16 \
    --quantization awq \          # å¦‚æœæ¨¡å‹é‡åŒ–è¿‡ï¼Œå¿…é¡»åŠ 
    --gpu-memory-utilization 0.85 \ # æ˜¾å­˜é¢„ç•™æ¯”ä¾‹ï¼Œè¶Šå¤§ KV Cache è¶Šå¤š
    --max-model-len 4096 \        # å¼ºåˆ¶æˆªæ–­ï¼Œé˜²æ­¢ OOM
    --max-num-seqs 128            # é™åˆ¶å¹¶å‘åºåˆ—æ•°
```

#### æ€§èƒ½è°ƒä¼˜

| æŒ‡æ ‡ | è¯´æ˜ | å¹³è¡¡ç­–ç•¥ |
|------|------|----------|
| **Throughput (ååé‡)** | å•ä½æ—¶é—´å¤„ç†çš„è¯·æ±‚æ•° | Batch size è¶Šå¤§ï¼Œååè¶Šé«˜ |
| **Latency (å»¶è¿Ÿ)** | å•ä¸ªè¯·æ±‚çš„å“åº”æ—¶é—´ | ä½†å»¶è¿Ÿå¯èƒ½å¢åŠ ï¼Œéœ€å¯»æ‰¾å¹³è¡¡ç‚¹ |

#### âš ï¸ é¿å‘æŒ‡å—

> **å¸¸è§é”™è¯¯ï¼š** é‡åˆ° `Request ignored` æŠ¥é”™ï¼Œé€šå¸¸æ˜¯ï¼š
> - `max-model-len` æ²¡è®¾é™åˆ¶
> - æ˜¾å­˜è¢« KV Cache æ’‘çˆ†äº†
> - éœ€è¦é™ä½ `gpu-memory-utilization` æˆ– `max-num-seqs`

---

### Phase 3: åç«¯æ¶æ„ (Backend & RAG)

**ä»»åŠ¡ï¼š** æ­å»ºä¸é˜»å¡çš„ APIï¼Œå®ç°æ‰“å­—æœºæ•ˆæœã€‚

**æŠ€æœ¯æ ˆï¼š** FastAPI, Uvicorn, LangChain / LlamaIndex

#### æ ¸å¿ƒæ¨¡å¼

- **Async/Await**: å¿…é¡»ä½¿ç”¨ `async def` å®šä¹‰æ¥å£ï¼Œè°ƒç”¨æ•°æ®åº“å’Œæ¨¡å‹æ—¶å¿…é¡» `await`
- **SSE (Server-Sent Events)**: æµå¼è¾“å‡ºçš„æ ‡å‡†åè®®

#### RAG é»„é‡‘é“¾è·¯

```mermaid
graph LR
    A[ç”¨æˆ·é—®é¢˜] --> B[Rewrite<br/>æ”¹å†™é—®é¢˜]
    B --> C[Retrieve<br/>æ··åˆæ£€ç´¢]
    C --> D[Rerank<br/>é‡æ’åº]
    D --> E[Generate<br/>ç”Ÿæˆç­”æ¡ˆ]
```

1. **Rewrite**: æ”¹å†™ç”¨æˆ·é—®é¢˜ï¼Œæå‡æ£€ç´¢å‡†ç¡®ç‡
2. **Retrieve**: æ··åˆæ£€ç´¢ï¼ˆVector + Keywordï¼‰
3. **Rerank (é‡æ’åº)**: ä½¿ç”¨ BGE-Reranker ç­‰å°æ¨¡å‹å¯¹æ£€ç´¢ç»“æœç²¾æ’ï¼ˆTop 50 â†’ Top 5ï¼‰
4. **Generate**: æ‹¼æ¥ Prompt é€å…¥ vLLM

**å½“å‰å®ç°ï¼š**

é¡¹ç›®å·²å®ç°åŸºç¡€çš„ RAG æµç¨‹ï¼ˆä½äº `src/api/main.py`ï¼‰ï¼š
- âœ… å‘é‡æ£€ç´¢ï¼ˆä½¿ç”¨ ChromaDBï¼‰
- âœ… ä¸Šä¸‹æ–‡æ‹¼æ¥
- âœ… vLLM é›†æˆ

**æ‰©å±•æ–¹å‘ï¼š**
```python
# åœ¨ src/api/main.py ä¸­æ‰©å±•
@app.post("/api/rag/chat")
async def chat_endpoint(request: ChatRequest):
    # 1. æ”¹å†™é—®é¢˜ï¼ˆå¾…å®ç°ï¼‰
    rewritten_query = await rewrite_query(request.query)
    
    # 2. æ£€ç´¢ï¼ˆå·²å®ç°ï¼‰
    docs = await retriever.retrieve(rewritten_query)
    
    # 3. é‡æ’åºï¼ˆå¾…å®ç°ï¼‰
    ranked_docs = await reranker.rerank(docs, top_k=5)
    
    # 4. ç”Ÿæˆï¼ˆå·²å®ç°ï¼‰
    response = await llm.generate(context=ranked_docs, query=request.query)
    
    return {"response": response}
```

---

### Phase 4: ç”Ÿäº§äº¤ä»˜ (Production & Ops)

**ä»»åŠ¡ï¼š** è¯æ˜ç³»ç»Ÿç¨³å¥ï¼Œç”¨æ•°æ®è¯´è¯ã€‚

**æŠ€æœ¯æ ˆï¼š** Docker, Locust (å‹æµ‹), Prometheus + Grafana

#### ç›‘æ§é‡ç‚¹

| æŒ‡æ ‡ | è¯´æ˜ | é˜ˆå€¼ |
|------|------|------|
| **gpu_cache_usage** | KV Cache ä½¿ç”¨ç‡ | å¦‚æœé•¿æœŸé«˜äº 95%ï¼Œè¯´æ˜éœ€è¦åŠ å¡æˆ–ä¼˜åŒ–æ¨¡å‹é•¿åº¦ |
| **request_latency** | è¯·æ±‚å»¶è¿Ÿ | P50 < 200ms, P99 < 1s |
| **throughput** | ååé‡ | æ ¹æ®ä¸šåŠ¡éœ€æ±‚è®¾å®š |

#### éƒ¨ç½²æ£€æŸ¥æ¸…å•

- [ ] æ¨¡å‹æƒé‡å·²åˆå¹¶ï¼ˆé LoRA Adapterï¼‰
- [ ] vLLM æœåŠ¡æ­£å¸¸å¯åŠ¨ï¼Œæ—  OOM é”™è¯¯
- [ ] FastAPI æ¥å£æ”¯æŒå¼‚æ­¥å’Œæµå¼è¾“å‡º
- [ ] RAG é“¾è·¯å®Œæ•´ï¼ˆRewrite â†’ Retrieve â†’ Rerank â†’ Generateï¼‰
- [ ] ç›‘æ§æŒ‡æ ‡å·²é…ç½®ï¼ˆGPU ä½¿ç”¨ç‡ã€å»¶è¿Ÿã€ååé‡ï¼‰
- [ ] å‹æµ‹é€šè¿‡ï¼ˆä½¿ç”¨ Locust è¿›è¡Œè´Ÿè½½æµ‹è¯•ï¼‰

---

## ğŸ“š ç›¸å…³èµ„æº

- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [LangChain æ–‡æ¡£](https://python.langchain.com/)
- [PEFT (LoRA) æ–‡æ¡£](https://huggingface.co/docs/peft/)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers/)

## ğŸ”§ å¸¸è§é—®é¢˜

### Q: è®­ç»ƒæ—¶å‡ºç°æ˜¾å­˜ä¸è¶³ï¼Ÿ
A: åœ¨ `config/train_config.yaml` ä¸­ï¼š
- å¯ç”¨ 4-bit é‡åŒ–ï¼š`load_in_4bit: true`
- å‡å°æ‰¹æ¬¡å¤§å°ï¼š`per_device_train_batch_size: 4`
- å¢åŠ æ¢¯åº¦ç´¯ç§¯ï¼š`gradient_accumulation_steps: 2`

### Q: vLLM å¯åŠ¨å¤±è´¥ï¼Œæç¤º OOMï¼Ÿ
A: åœ¨ `scripts/vllm.sh` ä¸­ï¼š
- é™ä½ `--gpu-memory-utilization`ï¼ˆå¦‚ 0.8ï¼‰
- å‡å° `--max-num-seqs`ï¼ˆå¦‚ 64ï¼‰
- å‡å° `--max-model-len`ï¼ˆå¦‚ 2048ï¼‰

### Q: å¦‚ä½•æ·»åŠ æ–°çš„æ–‡æ¡£åˆ°çŸ¥è¯†åº“ï¼Ÿ
A: 
1. å°†æ–‡æ¡£æ·»åŠ åˆ° `data/docs/` ç›®å½•
2. è¿è¡Œ `python src/core/ingest.py` é‡æ–°æ„å»ºå‘é‡åº“

### Q: å¦‚ä½•ä¿®æ”¹ API ç«¯å£ï¼Ÿ
A: 
- vLLM æœåŠ¡ï¼šä¿®æ”¹ `scripts/vllm.sh` ä¸­çš„ `--port`
- FastAPI æœåŠ¡ï¼šä¿®æ”¹ `scripts/fastapi.sh` ä¸­çš„ `--port`

## ğŸ“ å¼€å‘è¯´æ˜

### ä»£ç ç»“æ„è¯´æ˜

- `src/core/` - æ ¸å¿ƒåŠŸèƒ½æ¨¡å—ï¼Œå¯ç‹¬ç«‹ä½¿ç”¨
- `src/api/` - API æœåŠ¡å±‚ï¼Œä¾èµ– core æ¨¡å—
- `src/training/` - è®­ç»ƒç›¸å…³è„šæœ¬ï¼Œå¯ç‹¬ç«‹è¿è¡Œ
- `scripts/` - å¯åŠ¨è„šæœ¬ï¼Œæ”¯æŒç›¸å¯¹è·¯å¾„ï¼Œå¯åœ¨ä»»æ„ä½ç½®è¿è¡Œ

### æ‰©å±•å¼€å‘

1. **æ·»åŠ æ–°çš„æ£€ç´¢å™¨**ï¼šåœ¨ `src/core/` ä¸­åˆ›å»ºæ–°æ¨¡å—
2. **æ‰©å±• API æ¥å£**ï¼šåœ¨ `src/api/main.py` ä¸­æ·»åŠ è·¯ç”±
3. **è‡ªå®šä¹‰è®­ç»ƒæµç¨‹**ï¼šä¿®æ”¹ `src/training/train.py`

---

**License**: è§ [LICENSE](LICENSE) æ–‡ä»¶

