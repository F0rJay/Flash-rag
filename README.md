## Flash-RAG

Flash-RAG æ˜¯ä¸€ä¸ªåŸºäº **vLLM** çš„é«˜å¹¶å‘å‚ç›´é¢†åŸŸæ™ºèƒ½é—®ç­”å¼•æ“ï¼Œå½“å‰ä¸»è¦èšç„¦äº **æ³•å¾‹æ¡æ–‡å’¨è¯¢åŠ©æ‰‹** åœºæ™¯ã€‚

---

## é¡¹ç›®ç»“æ„

```text
Flash-RAG/
â”œâ”€â”€ config/                # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â””â”€â”€ train_config.yaml  # è®­ç»ƒä¸æ¨¡å‹ç›¸å…³çš„å…¨éƒ¨å‚æ•°
â”œâ”€â”€ datasets/                  # è®­ç»ƒ/è¯„æµ‹æ•°æ®
â”‚   â””â”€â”€ train.jsonl
â”œâ”€â”€ output/                # è®­ç»ƒè¾“å‡ºä¸æ—¥å¿—ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
â”œâ”€â”€ train.py               # è®­ç»ƒè„šæœ¬ï¼Œåªè´Ÿè´£é€»è¾‘ï¼Œä¸å†™æ­»å‚æ•°
â””â”€â”€ requirements.txt       # é¡¹ç›®ä¾èµ–
```

---

## å¿«é€Ÿå¼€å§‹

1. å®‰è£…ä¾èµ–ï¼š

```bash
pip install -r requirements.txt
```

2. å‡†å¤‡æ•°æ®ï¼ˆç¡®ä¿ `data/train.jsonl` å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®ï¼‰ã€‚

3. å¯åŠ¨è®­ç»ƒï¼š

```bash
python train.py
```

æ ¹æ®éœ€è¦ä¿®æ”¹ `config/train_config.yaml` å³å¯è°ƒæ•´æ¨¡å‹ã€æ•°æ®å’Œè®­ç»ƒå‚æ•°ã€‚

---

## ğŸš€ é¡¹ç›®å¼€å‘è¦ç‚¹é€ŸæŸ¥å¡

### æ ¸å¿ƒç›®æ ‡

æ‰“é€ ä¸€ä¸ª **ä½å»¶è¿Ÿã€é«˜å¹¶å‘ã€æ‡‚å‚ç›´é¢†åŸŸçŸ¥è¯†** çš„ç”Ÿäº§çº§ AI é—®ç­”ç³»ç»Ÿã€‚

---

### Phase 1: æ¨¡å‹ç‰¹è®­ (Training & Optimization)

**ä»»åŠ¡ï¼š** è®©æ¨¡å‹"æ‡‚è¡Œ"ä¸”"è½»é‡"ã€‚

**æŠ€æœ¯æ ˆï¼š** HuggingFace Transformers, PEFT, AutoGPTQ / BitsAndBytes

#### å…³é”®æ¦‚å¿µ

| æ¦‚å¿µ | è¯´æ˜ | å…³é”®å‚æ•° |
|------|------|----------|
| **LoRA (Low-Rank Adaptation)** | åªè®­ç»ƒæ—è·¯å°çŸ©é˜µï¼Œå¤§å¹…å‡å°‘è®­ç»ƒæˆæœ¬ | `r` (Rank, å¦‚ 8 æˆ– 16)<br>`target_modules` (é€šå¸¸æ¶µç›–æ‰€æœ‰ Linear layers) |
| **Merge Weights (æƒé‡åˆå¹¶)** | âš ï¸ **å¿…åšæ­¥éª¤ï¼** è®­ç»ƒå®Œå¿…é¡»å°† LoRA æƒé‡åˆå¹¶å›åº•åº§æ¨¡å‹ | ä¿å­˜ä¸ºç‹¬ç«‹çš„ `.safetensors` æ ¼å¼ |
| **Quantization (é‡åŒ–)** | æ¨è AWQ æ ¼å¼ï¼ˆæ¯” GPTQ å¯¹ vLLM æ”¯æŒæ›´å¥½ï¼‰ | å°†æ˜¾å­˜éœ€æ±‚ç åˆ° 1/3 |

#### âš ï¸ é¿å‘æŒ‡å—

> **é‡è¦ï¼š** åªæœ‰åˆå¹¶äº†æƒé‡ï¼Œæ¨ç†é€Ÿåº¦æ‰ä¼šå¿«ã€‚æŒ‚è½½ Adapter æ¨ç†åè€Œä¼šå˜æ…¢ã€‚

**è®­ç»ƒæµç¨‹ï¼š**
```bash
# 1. è®­ç»ƒ LoRA é€‚é…å™¨
python train.py

# 2. åˆå¹¶æƒé‡ï¼ˆå¿…é¡»ï¼ï¼‰
python merge.py

# 3. é‡åŒ–ï¼ˆå¯é€‰ï¼Œä½†æ¨èï¼‰
# ä½¿ç”¨ AutoGPTQ æˆ– AWQ å·¥å…·è¿›è¡Œé‡åŒ–
```

---

### Phase 2: æé€Ÿæ¨ç† (Inference Engine)

**ä»»åŠ¡ï¼š** æ¦¨å¹² GPU æ€§èƒ½ï¼Œè§£å†³æ˜¾å­˜ç“¶é¢ˆã€‚

**æŠ€æœ¯æ ˆï¼š** vLLM

#### æ ¸å¿ƒæœºåˆ¶

- **PagedAttention**: æ˜¾å­˜åˆ†é¡µç®¡ç†ï¼Œæ‹’ç»ç¢ç‰‡åŒ–

#### å¯åŠ¨å‚æ•°ç¤ºä¾‹

```bash
vllm serve \
    /path/to/merged_model \
    --host 0.0.0.0 \
    --port 8000 \
    --dtype bfloat16 \
    --quantization awq \          # å¦‚æœæ¨¡å‹é‡åŒ–è¿‡ï¼Œå¿…é¡»åŠ 
    --gpu-memory-utilization 0.85 \ # æ˜¾å­˜é¢„ç•™æ¯”ä¾‹ï¼Œè¶Šå¤§ KV Cache è¶Šå¤š
    --max-model-len 4096 \        # å¼ºåˆ¶æˆªæ–­ï¼Œé˜²æ­¢ OOM
    --max-num-seqs 128            # é™åˆ¶å¹¶å‘åºåˆ—æ•°
```

#### æ€§èƒ½è°ƒä¼˜

| æŒ‡æ ‡ | è¯´æ˜ | å¹³è¡¡ç­–ç•¥ |
|------|------|----------|
| **Throughput (ååé‡)** | å•ä½æ—¶é—´å¤„ç†çš„è¯·æ±‚æ•° | Batch size è¶Šå¤§ï¼Œååè¶Šé«˜ |
| **Latency (å»¶è¿Ÿ)** | å•ä¸ªè¯·æ±‚çš„å“åº”æ—¶é—´ | ä½†å»¶è¿Ÿå¯èƒ½å¢åŠ ï¼Œéœ€å¯»æ‰¾å¹³è¡¡ç‚¹ |

#### âš ï¸ é¿å‘æŒ‡å—

> **å¸¸è§é”™è¯¯ï¼š** é‡åˆ° `Request ignored` æŠ¥é”™ï¼Œé€šå¸¸æ˜¯ï¼š
> - `max-model-len` æ²¡è®¾é™åˆ¶
> - æ˜¾å­˜è¢« KV Cache æ’‘çˆ†äº†
> - éœ€è¦é™ä½ `gpu-memory-utilization` æˆ– `max-num-seqs`

---

### Phase 3: åç«¯æ¶æ„ (Backend & RAG)

**ä»»åŠ¡ï¼š** æ­å»ºä¸é˜»å¡çš„ APIï¼Œå®ç°æ‰“å­—æœºæ•ˆæœã€‚

**æŠ€æœ¯æ ˆï¼š** FastAPI, Uvicorn, LangChain / LlamaIndex

#### æ ¸å¿ƒæ¨¡å¼

- **Async/Await**: å¿…é¡»ä½¿ç”¨ `async def` å®šä¹‰æ¥å£ï¼Œè°ƒç”¨æ•°æ®åº“å’Œæ¨¡å‹æ—¶å¿…é¡» `await`
- **SSE (Server-Sent Events)**: æµå¼è¾“å‡ºçš„æ ‡å‡†åè®®

#### RAG é»„é‡‘é“¾è·¯

```mermaid
graph LR
    A[ç”¨æˆ·é—®é¢˜] --> B[Rewrite<br/>æ”¹å†™é—®é¢˜]
    B --> C[Retrieve<br/>æ··åˆæ£€ç´¢]
    C --> D[Rerank<br/>é‡æ’åº]
    D --> E[Generate<br/>ç”Ÿæˆç­”æ¡ˆ]
```

1. **Rewrite**: æ”¹å†™ç”¨æˆ·é—®é¢˜ï¼Œæå‡æ£€ç´¢å‡†ç¡®ç‡
2. **Retrieve**: æ··åˆæ£€ç´¢ï¼ˆVector + Keywordï¼‰
3. **Rerank (é‡æ’åº)**: ä½¿ç”¨ BGE-Reranker ç­‰å°æ¨¡å‹å¯¹æ£€ç´¢ç»“æœç²¾æ’ï¼ˆTop 50 â†’ Top 5ï¼‰
4. **Generate**: æ‹¼æ¥ Prompt é€å…¥ vLLM

**ç¤ºä¾‹ä»£ç ç»“æ„ï¼š**
```python
# FastAPI å¼‚æ­¥æ¥å£ç¤ºä¾‹
@app.post("/api/rag/chat")
async def chat_endpoint(request: ChatRequest):
    # 1. æ”¹å†™é—®é¢˜
    rewritten_query = await rewrite_query(request.query)
    
    # 2. æ£€ç´¢
    docs = await retriever.retrieve(rewritten_query)
    
    # 3. é‡æ’åº
    ranked_docs = await reranker.rerank(docs, top_k=5)
    
    # 4. ç”Ÿæˆ
    response = await llm.generate(context=ranked_docs, query=request.query)
    
    return {"response": response}
```

---

### Phase 4: ç”Ÿäº§äº¤ä»˜ (Production & Ops)

**ä»»åŠ¡ï¼š** è¯æ˜ç³»ç»Ÿç¨³å¥ï¼Œç”¨æ•°æ®è¯´è¯ã€‚

**æŠ€æœ¯æ ˆï¼š** Docker, Locust (å‹æµ‹), Prometheus + Grafana

#### ç›‘æ§é‡ç‚¹

| æŒ‡æ ‡ | è¯´æ˜ | é˜ˆå€¼ |
|------|------|------|
| **gpu_cache_usage** | KV Cache ä½¿ç”¨ç‡ | å¦‚æœé•¿æœŸé«˜äº 95%ï¼Œè¯´æ˜éœ€è¦åŠ å¡æˆ–ä¼˜åŒ–æ¨¡å‹é•¿åº¦ |
| **request_latency** | è¯·æ±‚å»¶è¿Ÿ | P50 < 200ms, P99 < 1s |
| **throughput** | ååé‡ | æ ¹æ®ä¸šåŠ¡éœ€æ±‚è®¾å®š |

#### éƒ¨ç½²æ£€æŸ¥æ¸…å•

- [ ] æ¨¡å‹æƒé‡å·²åˆå¹¶ï¼ˆé LoRA Adapterï¼‰
- [ ] vLLM æœåŠ¡æ­£å¸¸å¯åŠ¨ï¼Œæ—  OOM é”™è¯¯
- [ ] FastAPI æ¥å£æ”¯æŒå¼‚æ­¥å’Œæµå¼è¾“å‡º
- [ ] RAG é“¾è·¯å®Œæ•´ï¼ˆRewrite â†’ Retrieve â†’ Rerank â†’ Generateï¼‰
- [ ] ç›‘æ§æŒ‡æ ‡å·²é…ç½®ï¼ˆGPU ä½¿ç”¨ç‡ã€å»¶è¿Ÿã€ååé‡ï¼‰
- [ ] å‹æµ‹é€šè¿‡ï¼ˆä½¿ç”¨ Locust è¿›è¡Œè´Ÿè½½æµ‹è¯•ï¼‰

---

## ğŸ“š ç›¸å…³èµ„æº

- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [LangChain æ–‡æ¡£](https://python.langchain.com/)
- [PEFT (LoRA) æ–‡æ¡£](https://huggingface.co/docs/peft/)

